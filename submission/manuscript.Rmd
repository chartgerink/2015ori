```{r preparation, echo = FALSE}
suppressPackageStartupMessages(if(!require(pROC)){install.packages('pROC', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(if(!require(knitr)){install.packages('knitr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(if(!require(foreign)){install.packages('foreign', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(foreign))
suppressPackageStartupMessages(if(!require(latex2exp)){install.packages('latex2exp', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(if(!require(ggplot2)){install.packages('ggplot2', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(if(!require(gridExtra)){install.packages('gridExtra', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(if(!require(xtable)){install.packages('xtable', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(xtable))
suppressPackageStartupMessages(if(!require(httr)){install.packages('httr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(httr))
suppressPackageStartupMessages(if (!require(car)){install.packages('car', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(car))
suppressPackageStartupMessages(if (!require(lsr)){install.packages('lsr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(lsr))
suppressPackageStartupMessages(if (!require(data.table)){install.packages('data.table', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(if (!require(dplyr)){install.packages('dplyr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(if (!require(reshape2)){install.packages('reshape2', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(if (!require(metafor)){install.packages('metafor', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(metafor))

```

# Introduction

Statistical methods provide one way to improve detection of data fabrication in empirical research. Humans are notoriously bad at understanding and estimating randomness [@10.1126/science.185.4157.1124;@10.1037/h0031322;@10.1037/1082-989X.5.2.241], which could manifest itself in the fundamentally probabilistic data they try to fabricate. 
When data are fabricated, principles of statistics and randomness are easily violated at the univariate level, bivariate level, trivatiate level, or beyond [@Haldane1948-nm]. Based on this idea, statistical methods that investigate whether the reported data are feasible under the theoretically probabilistic processes can be used to detect potential data fabrication.

```{r echo=FALSE}
m <- 50
sd <- 10
n <- 100

set.seed(1234)
x1_1 <- replicate(n = 10, expr = rnorm(n, m, sd))
x1_2 <- replicate(n = 10, expr = rnorm(n, m, sd))

ex1_1em <- apply(x1_1, 2, mean)
ex1_1esd <- apply(x1_1, 2, sd)
ex1_1cm <- apply(x1_2, 2, mean)
ex1_1csd <- apply(x1_2, 2, sd)
ex1_1tp <- NULL
for (i in 1:dim(x1_1)[2]) ex1_1tp[i] <- t.test(x = x1_1[,i], y = x1_2[,i], var.equal=TRUE)$p.value
ex1_1tp <- unlist(ex1_1tp)

set.seed(1234)
x2_1em <- replicate(n = 10, expr = m + runif(1, 0, 20))
x2_2cm <- replicate(n = 10, expr = m + runif(1, 0, 20))
x2_1esd <- replicate(n = 10, expr = sd + runif(1, 0, 1.5))
x2_2csd <- replicate(n = 10, expr = sd + runif(1, 0, 1.5))

sdpooled <- ((n - 1) * x2_1esd^2 + (n - 1) * x2_2csd^2)/(n * 2 - 2)
tval <- (x2_1em - x2_2cm) / sdpooled
ex2_1tp <- pt(abs(tval), n * 2 - 2, lower.tail = FALSE) * 2

# Just saving for later use in theory section
# set.seed(1234)
# std_var(n = rep(100, length(x2_1esd) * 2), sds = c(x2_1esd, x2_2csd), iter = 10000)

df <- data.frame(study = sprintf("Study %s", 1:10),
         sprintf("%s (%s)", round(ex1_1em, 3), round(ex1_1esd, 3)),
         sprintf("%s (%s)", round(ex1_1cm, 3), round(ex1_1csd, 3)),
         ex1_1tp,
         sprintf("%s (%s)", round(x2_1em, 3), round(x2_1esd, 3)),
         sprintf("%s (%s)", round(x2_2cm, 3), round(x2_2csd, 3)),
         ex2_1tp)
names(df) <- c("Study",
        "$M_E$ ($SD_E$) [S1]",
        "$M_C$ ($SD_C$) [S1]",
        "P-value [S1]",
        "$M_E$ ($SD_E$) [S2]",
        "$M_C$ ($SD_C$) [S2]",
        "P-value [S2]")
```


```{r table_excess, echo = FALSE}
knitr::kable(df, digits = 3, caption = "Examples of means and standard deviations for a continuous outcome in genuine- and fabricated randomized clinical trials. Set 1 (S1) is randomly generated data under the null hypothesis of random assignment (assumed to be the genuine process), whereas Set 2 (S2) is generated under excessive consistency with equal groups. Each trial condition contains 100 participants. The $p$-values are the result of independent $t$-tests comparing the experimental and control conditions within each respective set.")
```

Statistical methods to detect data fabrication, although developed to quantify suspicions in a specific paper, could be applied to screen multiple papers. The application of such methods can be (semi-)automated if data are available in a machine-readable format that one of the statistical methods can be applied to. An example of such a potential case for mass application of using statistics to detect (potential) data fabrication is in the ClinicalTrials.gov database, where baseline measures across randomized groups are readily available for download and subsequent analysis [@10.3897/rio.1.e7462]. 

Nonetheless, prior to applying statistical methods to flag potentially fabricated results, investigating whether such methods perform well enough in detecting data fabrication is required for responsible application. 
We hardly know how researchers might go about fabricating data. Cases such as Stapel, Fuji, Smeesters, and Sanna provide some insights, but are highly pre-selected (i.e., those who got caught/confessed) and as such, may be systematically biased. Relatively extensive descriptions in rare and partial autobiogrophical accounts provide little insight into the actual data fabrication process, except for the setting where it might take place [e.g., late at night when no one is around; @stapel_book]. Additionally, the performance of methods to detect data fabrication is highly dependent on the unknown prevalence of data fabrication and the power to actually detect data fabrication. Given that we do not know how researchers might fabricate data, the diagnosticity of these methods cannot realistically be assessed.

Throughout this paper, we inspect statistical methods to detect potential data fabrication that can be applied to summary statistics (Study 1) or raw data (Study 2). 
Even though structure and contents of data can look different depending on the structure of a study and the measures, there are certain common characteristics of empirical results and the underlying raw data that can be inspected. For example, summary statistics frequently include means, standard deviations, test-statistics, and $p$-values. Raw data frequently contain at least some variables measured at a interval- or ratio scale [@10.1126/science.103.2684.677]. Such common characteristics allow for the development of generic statistical methods that can be applied across a varied set of results to screen for fabricated data. For each study, we review the theory of the specific methods we apply. However, the reviewed methods are not exhaustive of all methods available to test for potential data fabrication in empirical data [see also, @buyse1999;@10.7287/peerj.preprints.2400v1;@10.7287/peerj.preprints.2064v1;@sprite-heathers].

# Study 2 - detecting fabricated Stroop data


### Data analysis




Study 1 showed that effect sizes are a potentially valuable tool to detect data fabrication, which we exploratively replicate in Study 2. Based on the data sets, we computed effect sizes for the Stroop effect based on the Many Labs 3 scripts ([osf.io/XXXX](https://osf.io/XXXX)). Using a $t$-test of the difference between the congruent and incongruent conditions ($H_0:\mu=0$) we computed the $t$-value and its constituent effect size as a correlation using 
$$
r=\sqrt{\frac{\frac{t^2}{df_2}}{\frac{t^2}{df_2}+1}}
$$

Similar to Study 1, we computed the AUROC for each of these statistical methods to detect data fabrication. To recapitulate, if $AUROC=.5$, correctly classifying a randomly drawn dataset in this sample is equal to a coin flip. We follow regard any AUROC value $<.7$ as poor for detecting data fabrication, $.7\leq$ AUROC $<.8$ as fair, $.8\leq$ AUROC $<.9$ as good, and AUROC $\geq.9$ as excellent [@10.1093/jpepsy/jst062].


## Results

```{r, echo = FALSE}
dat <- read.csv(sprintf('%sdata/study_02/ml3_fabricated_processed_collated.csv', x12))

apply(dat[, grepl(names(dat), pattern = "(_p|one_min_es_r|p_)")],
   2,
   function(x)
   {
    y <- pROC::roc(response = dat$type, x, direction = "<")
    return(y$auc[1])
   }
)

tp <- matrix(NA, ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
fp <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
fn <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
tn <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)

temp <- names(dat)[grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")]

j <- 1

for (variables in which(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")))
{
 i <- 1 
 
 for (threshold in seq(0, 1, .01))
 {
  tp[i,j] <- sum(dat[, variables] <= threshold & dat$type == 'fabricated', na.rm = TRUE)
  fp[i,j] <- sum(dat[, variables] <= threshold & dat$type == 'ml3', na.rm = TRUE)
  fn[i,j] <- sum(dat[, variables] > threshold & dat$type == 'fabricated', na.rm = TRUE)
  tn[i,j] <- sum(dat[, variables] > threshold & dat$type == 'ml3', na.rm = TRUE)
  
  i <- i + 1
 }
 
 j <- j + 1
}

# loop through matrix for proper comps

tpr <- matrix(NA, ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
fpr <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
fnr <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
tnr <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)

ppv <- matrix(NA, ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
fdr <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
npv <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)
false_or <- matrix(NA,ncol = sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")), nrow = 101)

for (i in 1:sum(grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")))
{
 tpr[, i] <- tp[, i] / (tp[, i] + fn[, i])
 fnr[, i] <- fn[, i] / (tp[, i] + fn[, i])
 fpr[, i] <- fp[, i] / (fp[, i] + tn[, i])
 tnr[, i] <- tn[, i] / (fp[, i] + tn[, i])
 
 ppv[, i] <- tp[, i] / (fp[, i] + tp[, i])
 fdr[, i] <- fp[, i] / (fp[, i] + tp[, i])
 npv[, i] <- tn[, i] / (tn[, i] + fn[, i])
 false_or[, i] <- fn[, i] / (tn[, i] + fn[, i])
}
fpr <- as.data.frame(fpr)
tpr <- as.data.frame(tpr)
fnr <- as.data.frame(fnr)
tnr <- as.data.frame(tnr)

ppv <- as.data.frame(ppv)
fdr <- as.data.frame(fdr)
npv <- as.data.frame(npv)
false_or <- as.data.frame(false_or)

names(fpr) <- temp
names(tpr) <- temp
names(fnr) <- temp
names(tnr) <- temp

names(ppv) <- temp
names(fdr) <- temp
names(npv) <- temp
names(false_or) <- temp

height = (tpr[-1, ] + tpr[-dim(tpr)[1], ])/2
width = apply(fpr, 2, diff) # = diff(rev(omspec))
AUC_res <- apply(height * width, 2, sum)

temp <- names(dat)[grepl(names(dat), pattern = "(_p|p_|one_min_es_r|fisher_combination)")]
Method <- c("NBL, congruent means",
      "NBL, congruent SDs",
      "NBL, incongruent means",
      "NBL, incongruent SDs",
      "Terminal digit analysis, congruent means",
      "Terminal digit analysis, congruent SDs",
      "Terminal digit analysis, incongruent means",
      "Terminal digit analysis, incongruent SDs",
      "Variance analysis, congruent condition",
      "Variance analysis, incongruent condition",
      "Multivariate association means and SDs, congruent condition",
      "Multivariate association means and SDs, incongruent condition",
      "Multivariate association means, across conditions",
      "Multivariate association SDs, across conditions",
      "Fisher combination of terminal, variance, and multivariate",
      "Effect size ($1-r$)")

df <- data.frame(Method = Method,
         AUROC = AUC_res)
```

### Performance of NBL to detect data fabrication

```{r table_nbl, echo=FALSE}
knitr::kable(df[1:4,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using the Newcomb-Benford law (NBL) to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

```{r}
ggplot(dat, aes(x = benford_congr_m_p)) + geom_density(aes(fill = type)) + xlim(0, .00001)
```

### Performance of terminal digit analysis to detect data fabrication

```{r table_terminal, echo=FALSE}
knitr::kable(df[5:8,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using terminal digit analysis to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of variance analysis to detect data fabrication

```{r table_variance_raw, echo=FALSE}
knitr::kable(df[9:10,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using variance analysis to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of multivariate associations to detect data fabrication

```{r table_multi, echo=FALSE}
knitr::kable(df[11:14,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using multivariate associations to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of combining 

```{r table_fisher_raw, echo=FALSE}
knitr::kable(df[15,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using variance analysis to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of effect sizes

```{r table_es_raw, echo=FALSE}
knitr::kable(df[16,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using variance analysis to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

```{r, echo=FALSE, out.width='100%', fig.cap="The effect size distributions from Many Labs 3 and those fabricated by the participants"}
ggplot(dat, aes(x = es_r)) + geom_density(aes(fill = type), alpha = .3)
# > min(dat$es_r[dat$type == 'fabricated'])
# [1] 0.294654
# > max(dat$es_r[dat$type == 'ml3'])
# [1] 0.3164714
```

## Discussion

<!-- # General discussion -->

<!-- The effectiveness of detection mechanisms and their consequences, hence their expected deterrence, is exacerbated in the digital age by the availability of public discussion platforms such as [PubPeer (https://pubpeer.com)](https://pubpeer.com). -->
<!-- PubPeer serves as an "online journal club" where anyone can discuss articles. Authors are notified when someone leaves a response, providing them with the possibility to respond. Such a platform allows for public discussion of the paper, including discussion of reanalyses, methodology, availability of materials, etc. When detection mechanisms are freely available to use, they can lead to (a surge of) comments when applied by users. Recently, in 2016, one user (the main author of this paper) used `statcheck` software to report potential statistical reporting inconsistencies for 50,000 psychology articles, which led to a large discussion about (automated) online comments [@10.1038/540151a]. The impact of such new possibilities is not to be underestimated, although its potential to constructively contribute to the scientific discussion should also not be [@10.1038/540007b]. Nonetheless, the `statcheck` software was well validated prior to this application [@10.3758/s13428-015-0664-2] and the same principle applies to the application of statistical methods to detect (potential) data fabrication. -->

<!-- Effect sizes good way to detect, but based on unbiased effect sizes! ML is unbiased, published lit isn't -->

<!-- ## `ddfab` package -->

<!-- All the methods used in this paper are also implemented in the open source R package `ddfab`. -->

<!-- However, these results indicate that application of these methods should be informed and as a screening method. -->

<!-- ## Study fallout -->

<!-- While conducting Study 2 reported in this paper, there was considerable criticism from several parties. -->

<!-- We -->

<!-- do something with this
https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-18
also shows effects are important and exaggerated by fabricators -->

# Session info

```{r}
sessionInfo()
```

# References
