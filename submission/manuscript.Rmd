---
title: "Ecological performance of detecting data fabrication"
author: "Chris HJ Hartgerink, Jan G Voelkel, Jelte M Wicherts, Marcel ALM van Assen"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
  word_document: default
csl: ../bibliography/apa.csl
bibliography: ../bibliography/library2.bib
---

```{r preparation, echo = FALSE}
suppressPackageStartupMessages(if(!require(pROC)){install.packages('pROC', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(if(!require(foreign)){install.packages('foreign', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(foreign))
suppressPackageStartupMessages(if(!require(latex2exp)){install.packages('latex2exp', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(if(!require(ggplot2)){install.packages('ggplot2', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(if(!require(gridExtra)){install.packages('gridExtra', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(if(!require(xtable)){install.packages('xtable', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(xtable))
suppressPackageStartupMessages(if(!require(httr)){install.packages('httr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(httr))
suppressPackageStartupMessages(if (!require(car)){install.packages('car', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(car))
suppressPackageStartupMessages(if (!require(effects)){install.packages('effects', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(effects))
suppressPackageStartupMessages(if (!require(lsr)){install.packages('lsr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(lsr))
suppressPackageStartupMessages(if (!require(data.table)){install.packages('data.table', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(if (!require(dplyr)){install.packages('dplyr', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(if (!require(reshape2)){install.packages('reshape2', repos = 'http://cran.us.r-project.org')})
suppressPackageStartupMessages(library(reshape2))

# Add ../ before running rmarkdown, or vice versa when not running there
x12 <- '../'
# x12 <- ''

source(sprintf('%sfunctions/digit_analysis.R', x12))
source(sprintf('%sfunctions/std_var.R', x12))

ml_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
qualtrics_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
dat_summary_file <- sprintf('%sdata/study_01/study1_res.csv', x12)
file <- sprintf('%sdata/study_01/raw_summary_results_fabrication_qualtrics.csv', x12)
res_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
ml_dat_file <- sprintf('%sdata/study_01/anchoring_ml/chjh ml1_anchoring cleaned.sav', x12)
summary_stat_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
ml3_dat_file <- sprintf('%sdata/study_02/study_02-ml3_stroop/StroopCleanSet.csv', x12)
pdf_file <- sprintf('%sarchive/gender_interaction.pdf', x12)

# Set the number of iterations to use in calculations
iter <- 100000

tabnr <- 1
fignr <- 1
```

Any field of empirical inquiry is faced with cases of scientific misconduct at some point, either in the form of fabrication, falsification or plagiarism (FFP). Psychology was faced with Stapel; medical sciences were faced with Poldermans and Macchiarini; life sciences were faced with Voignet. These are just a few examples of misconduct cases in the last decade. Overall, an estimated 2\% of all scholars have admitted to falsifying or fabricating research results at least once [@10.1371/journal.pone.0005738] and this is likely to be an underestimate due to socially desirable responses. The detection rate of data fabrication is likely to be even lower; for example, only around a dozen cases are discovered in the United States and the Netherlands, despite covering several hundreds of thousands of researchers. At best, this amounts to a detection rate far below 1\% of those 2\% who admit to fabricating data --- the tip of a seemingly much larger iceberg.

In order to stifle attempts at data fabrication, improved detection of fabricated data is considered to deter such harmful attempts. 
Although deterrence theory dates back to the middle of the 17th century [@leviathan], its implementation has not occurred equally across fabrication, falsification, and plagiarism. Basically, deterrence theory stipulates that with increased risk of detection, the utility of scientific misconduct (for this context) will decrease and therefore fewer people will engage in such behaviors. This principle of deterrence has been implemented with plagiarism scanners, a development that already started a long time ago [e.g., @10.1109/13.28038]. In the last decade, detecting image manipulation has become one of the few forms of detecting data fabrication. The Journal of Cell Biology scans each submitted image for potential manipulation [@The_Journal_of_Cell_Biology2015-vh], which greatly increases the risk of detecting (blatant) image manipulation. More recently, algorithms have been developed to automate the scanning of images for (subtle) manipulations [@10.1007/s11948-016-9841-7]. These developments in detecting image manipulation have increased detection risk during the pre-publication and post-publication phase by improving detection mechanisms and increasing the understanding of how images might be manipulated. Moreover, their application also helps researchers systematically evaluate research articles to estimate the extent of the problem of image manipulation [4\% of all papers are estimated to contain manipulated images, @10.1128/mBio.00809-16]. 

Statistical methods can provide one way to improve detection of data fabrication in empirical research. Humans are notoriously bad at understanding and estimating probabilities [e.g., @10.1126/science.185.4157.1124;@10.1037/h0031322], which could manifest itself in the fundamentally probabilistic data they try to fabricate. 
That researchers do not understand probabilistic processes also presents itself in false interpretation of genuine research data [@10.3758/BF03213921;@10.1007/s11336-015-9446-0;@10.7326/0003-4819-130-12-199906150-00008]. When data are fabricated, probabilistic principles are easily violated if forgotten at the univariate level, bivariate level, trivatiate level, or beyond [@Haldane1948-nm]. Based on this idea, statistical methods that investigate whether the reported data are feasible under the theoretically probabilistic processes can be used to detect potential data fabrication.

The application of such statistical methods to detect data fabrication has occurred in several cases in recent years and has potential for future application beyond a case-basis. 
For example, problems in papers by Fuji were highlighted with statistical methods [@10.1111/j.1365-2044.2012.07128.x;@10.1111/anae.13126], resulting in 183 retractions [@oransky2015]. In this case, baseline measures across randomized groups were examined for too little variation. Random assignment should introduce a certain degree of random error that might be missed by a human fabricator, misestimating the probabilistic process that generates such error. Another two cases are those of Sanna and Smeesters, where fabricated data were also detected with statistics [@10.1177/0956797613480366]. These cases inspected the variance of variances (i.e., the second, or meta level variance). Once again, too little variation was what revealed problems in these data [other examples include @lacour-complaint;@foerster-complaint]. These methods, although developed in a case-setting, need not be limited to cases. The application of such methods can be (semi-)automated if data are available in a machine-readable format that one of the statistical methods can be applied to. An example of such a potential case for mass application of using statistics to detect (potential) data fabrication is in the ClinicalTrials.gov database, where baseline measures across randomized groups are readily available for download and subsequent analysis [@10.3897/rio.1.e7462].

Nonetheless, considering the potential harm of applying statistical methods to flag potentially problematic results, it needs to be sorted out whether such methods function well enough to make it responsible to apply them. 
We hardly know how researchers might go about fabricating data. Cases such as Fuji, Smeesters, and Sanna provide some insights, but are highly pre-selected (i.e., those who got caught/confessed) and as such, systematically biased. Relatively extensive descriptions in rare and partial autobiogrophical accounts provide little insight into the actual data fabrication process, except for the setting where it might take place [e.g., late at night when no one is around; @stapel_book]. However, trustworthiness of these accounts can be called into question. Additionally, the performance of methods to detect data fabrication is highly dependent on the unknown prevalence of data fabrication and the power to actually to detect data fabrication. Given that we do not know how researchers might fabricate data, the diagnosticity of these methods cannot realistically be simulated.

The effectiveness of detection mechanisms and their consequences, hence their expected deterrence, is exacerbated in the digital age by the increased usage of public online discussion platforms such as [PubPeer (https://pubpeer.com)](https://pubpeer.com). 
PubPeer serves as an "online journal club" where anyone can discuss articles. Authors are notified when someone leaves a response, providing them with the possibility to respond. Such a platform allows for public discussion of the paper, including discussion of reanalyses, methodology, availability of materials, etc. When detection mechanisms are freely available to use, they can lead to (a surge of) comments when applied by users. Recently, in 2016, one user (the main author of this paper) used `statcheck` software to report potential statistical reporting inconsistencies for 50,000 psychology articles, which led to a large discussion about (automated) online comments [@10.1038/540151a]. The impact of such new possibilities is not to be underestimated, although its potential to constructively contribute to the scientific discussion should also not be [@10.1038/540007b]. Nonetheless, the `statcheck` software was well validated prior to this application [@10.3758/s13428-015-0664-2] and the same principle applies to the application of statistical methods to detect (potential) data fabrication.

# Theoretical framework

Throughout this paper, we inspect statistical methods to detect potential data fabrication that can be applied to (1) summary results or (2) raw data. 
Even though structure and contents of data can look different depending on the structure of a study and the measures, there are certain common characteristics of empirical results and the underlying raw data that can be inspected. For example, summary results frequently include means, standard deviations, test-statistics, and $p$-values. Raw data frequently contain at least some variables measured at a interval- or ratio scale [@10.1126/science.103.2684.677]. Such common characteristics allow for the development of generic statistical methods that can be applied across a varied set of results to screen for problematic data. We review the theoretical framework of the specific methods we apply throughout this paper, but these are not exhaustive of all methods available to test for potential problems in empirical data [see also @buyse1999;@10.7287/peerj.preprints.2400v1;@10.7287/peerj.preprints.2064v1;@sprite-heathers].

## Detecting data fabrication in summary results

### P-value analysis

The distribution of one $p$-value depends on various parameters, but there are specific theoretical boundary conditions to this distribution [@fisher1925]. Based on the population effect size, the precision of the estimate, and the observed effect size, the $p$-value distribution can be uniform (when then null hypothesis is true) or right-skewed (when the alternative hypothesis is true). By extension, any other distribution, after taking into account sampling error, is theoretically suspect. For example, a distribution might show a bump when authors $p$-hack [e.g., Figure 1 in @10.7717/peerj.1935], but fabricators might create other non-uniform or non-right-skewed distributions, failing to take into account the theoretical boundary conditions.

In order to test whether observed $p$-values transgress the boundary condition, we previously proposed an adaptation of Fisher's method [@10.1186/s41073-016-0012-9]. This adaptation is a simple reversal of the Fisher method [@fisher1925], which was originally introduced as a simple meta-analytic test to indicate presence of an effect; this test is computed as 
$$
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(p_i)
$$
where it tests for right-skew (i.e., more smaller $p$-values than larger $p$-values) across the $k$ number of $p$-values. Reversing the Fisher method results in 
$$
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(1-\frac{p_i-t}{1-t})
$$
where it now tests for left-skew (i.e., more larger $p$-values than smaller $p$-values) across the $k$ number of $p$-values that falls above the threshold $t$. This threshold is added and can be set to anything. Given the frequent misinterpretation of $p$-values [e.g., as the probability of an effect, @10.1053/j.seminhematol.2008.04.003;@10.1136/bmj.311.7003.485], researchers who fabricate nonsignificant data might forget to fabricate a uniform $p$-value distribution and generate only large $p$-values when fabricating nonsignificant results. In that case, it makes  sense to set $t=.05$ or $t=.10$, in order to include only nonsignificant test results. Upon writing this paper, it became clear to us that this is similar to the operating principle of Carlisle's method testing for excessive homogeneity across baseline measurements in Randomized Clinical Trials [RCTs; @10.1111/anae.13938;@10.1111/j.1365-2044.2012.07128.x;@10.1111/anae.13126].

Despite this test being useful for detecting data anomalies in nonsignificant $p$-values, two clear exceptions should be taken into account: (1) model-fit tests and (2) wrongly specified one-tailed tests. When model-fit tests are used, these can quickly result in high $p$-values because of model saturation (i.e., where the null hypothesis is good fit and the alternative is bad fit). For properly specified one-tailed tests, the $p$-value distribution is right-skewed. When wrongly specified, this distribution is reversed and becomes left-skew. These two exceptional cases should not be forgotten during the application of this test, but are irrelevant for the rest of this paper due to our study design (see methods of both studies).

### Variance analysis

Variance- or standard deviation estimates are typically reported to indicate spread, but just like the mean there should be sampling error in this estimate proportionate to the sample size (i.e., the variance of the variances). A variance estimate follows a $\chi^2$-distribution, which is dependent on the sample size [p. 445; @hogg-tanis]; that is
$$
z^2_j\sim\left(\frac{\chi^2_{N_j-1}}{N_j-1}\right)/MS_w
$$
where $N_j$ is the sample size of the $j$th group and $MS_w$ is the normalizing constant resulting in a standardized variance $z^2_j$. The normalizing constant $MS_w$ is computed as 
$$
MS_w=\frac{\sum\limits^k_{j=1}(N_j-1)s^2_j}{\sum\limits^k_{j=1}(N_j-1)}
$$
where $s^2_j$ is the variance in the $j$th group. If simulating (see next paragraph), the simulated variances are used to compute $MS_w$. 

Assuming that the observed variances are from the same population distribution, the expected spread of reported standardized variances can be simulated. By repeatedly drawing (standardized) variance estimates for $j$ groups and then computing their spread allows for approximation of the sampling distribution. Spread can be operationalized in various ways, such as the standard deviation of the variances (denoted in this paper as $SD_z$) or as the range of the variances (denoted as $max-min_z$). If there is reason to believe that the observed variances come from heterogeneous populations, subgrouping (i.e., one variance analysis per subgroup) is necessary to prevent false results.

Subsequently, the observed sampling variance (or range) can be compared to the expected sampling distribution. Too consistent results would indicate potential anomalies in the reported data [@10.1177/0956797613480366]. For example, if four independent samples all yield the variance $2.22$, this could be considered excessively consistent when the probability that this amount of consistency (or more) is less than 1 out of 1000 in truly random samples.

### Effect size analysis

A fabricator might present unrealistically large effects, forgetting the exponential relation between the effect size measure and the implied correlation. From our own experience, and anecdotal evidence elsewhere [@10.1016/0197-24569190037-M], large effects have previously raised initial suspicions. The size of a large effect can quickly become unrealistic; $d=1.2$ already implies a correlation of `r round(sqrt((1.2^2/(1.2^2+4))), 2)` between the dependent- and independent measures. More generally, the relation between the implied correlation and the effect size tends to be exponential (see Figure `r fignr`) depicts. 

```{r d2, fig.cap="The relation between Cohen's d effect size and its direct transformation, the correlation r (or vice versa). If effects d>2 are fabricated, the implied correlation between the dependent- and independent measure(s) is approximately .6.", out.width='100%', echo=FALSE}
d2 <- seq(0, 10, .01)

plot(sqrt(d2^2/(d2^2+4)), x = d2,
     type='l', xlab = "Cohen's d", ylab = "Correlation r")

fignr <- fignr + 1
```

Taking the observed effect size and transforming it into a correlation, allows for an easy way to assess how extreme the presented result is. One minus the observed correlation can be used as a measure for extreme effects (i.e., $1-r$); as a heuristic, it can be regarded as a $p$-value. That is, this measure too ranges from zero to one and the more extreme the effect size, the smaller the value. This method specifically looks at situations where fabricators would want to fabricate the existence of an effect (not the absence of one).

## Detecting data fabrication in raw data

### Digit analysis

Raw data can contain ratio- or interval scale measures that, under specific conditions, are subject to mathematical properties. These properties pertain either to the leading (first) digit (e.g., the 1 in 123.45) or the terminal (last) digit (e.g., the 5 in 123.45). By analyzing these leading- and terminal digits for deviations from such mathematical properties, it might be possible to screen for problematic data. These properties can be extended to sequences of digits, but in this article we focus on leading digit analysis (i.e., Benford's law) and terminal digit analysis to detect potentially problematic data.

#### Newcomb-Benford law

The Newcomb-Benford law [NBL; @10.2307/2369148;@10.2307/984802] states that leading digits do not have an equal probability of occuring in various cases. A leading digit is the left-most digit of a numeric value, where a digit is any of the nine natural numbers ($1,2,3,...,9$). The distribution of the leading digit, according to the NBL is
$$
P(d)=log_{10}\frac{1+d}{d}
$$
where $d$ is the natural number of the leading digit and $P(d)$ is the probability of $d$ occurring. This law has been empirically observed in a variety of cases, ranging from population data to the number of rivers in an area [@10.2307/984802]. Models that delineate what kinds of measures do- and do not adhere to NBL are non-trivial to compose.

For ratio scale measures that are scale and base invariant, the NBL often applies empirically but for measures other than those it is non-evident whether they follow the NBL [@10.2307/2246134;@
10.1214/11-PS175]. Scale- and base invariance indicate that the results of the measure do not change if the measure is multiplied by constant $c$ or transformed into a log scale of variable base. However, by extension, deviations from a ratio scale that is base- and scale invariant are likely to cause the  NBL to not be applicable. For example, if a ratio scale is truncated (either with a minimum or maximum) it is already sufficient to cause a violation of the NBL [@10.1515/9781400866595-011]. Moreover, a base- and scale invariant ratio measure implies adherence to the NBL [@10.1090/S0002-9939-1995-1233974-8], but does not necessitate that the leading digits follow the NBL. As such, empirical support that the numbers under investigation, in fact, follow the NBL is of importance before using the NBL to detect potential problems.

Despite the limitations of the NBL, this property has been applied to detect financial fraud [e.g., @10.2307/27643897] or voting fraud [e.g., @durtschi2004effective] and also to detect problems in scientific data. Previously, the NBL has been tested on 20 falsified anesthesia papers, detecting 18 as problematic [@10.1007/s00101-012-2029-x], or to test coefficients reported in economics journals [@10.1111/j.1468-0475.2009.00475.x]. The NBL is typically used to compute the expected values, which are then compared to the observed values using a $\chi^2$-test. However, the outcomes have not been validated to test the performance of the method.

The performance of the NBL to detect problematic data is not evidential. Based on the NBL, Obama's 2008 election was rigged [@Deckert]. Financial data is likely to be ridden with digit preferences, for example due to price setting, causing false flags. Additionally, scientific data do not necessarily follow the NBL, especially in the social sciences, because data are relatively sparse and often the result of human thought, which has previously been addressed as a reason why digits do not follow the NBL [@durtschi2004effective]. Moreover, people might even be sensitive to fabricating data that are in line with the NBL [@10.1080/02664760601004940], potentially depending on whether fabricators generate or select values [@Burns2009].

#### Terminal digit analysis

Terminal digit analysis is based on the principle that the rightmost digit contains most of the measurement error in the number [@10.1080/08989629508573866;@10.1080/03610919608813325]. For example, when someone's height is measured in micrometers, measurement one might be 1.848 metres, and a second measurement 1.841, etc. If the true height is 1.845 metres, it is a logical consequent that the terminal digit is more likely to be adjusted by measurement error (i.e., someone is unlikely to be two or zero metres tall due to measurement error). This is in essence a consequent of classical test theory, where each measurement is thought of as measuring a true score and random error [@10.1016/0022-2496(66)90002-2].

As such, the rightmost digit can be expected to be uniformly distributed if sufficient precision is provided [@10.1080/08989629508573866]. For our purposes, sufficient precision is determined as the terminal digit being at least the third leading digit. As such, if height measurements are reported in 1.8 metres, there are only three leading digits and there is insufficient precision. However, "1.84" contains three leading digits and is regarded as sufficient. Note also that this implies that Likert scales are wholly inappropriate for terminal digit analysis.

Terminal digit analysis is conducted with a $\chi^2$-test on the digit occurrence counts. As such, it compares the observed frequencies with the expected uniform frequencies. Before applying this method is important to consider the applicability of the principle that the last digit contains the most measurement error [@10.14293/S2199-1006.1.SOR-STAT.AFHTWC.v1] because it is central to the analysis. For example, if it is reasonable to expect there are digit preferences in genuine data, this method is unlikely to differentiate between potentially problematic and genuine data.

### Multivariate associations

True data occur within a web of relations, which can be observed in genuine data and easily forgotten in fabricated data. The multivariate relations between different variables arise from stochastic processes and are not readily known and therefore difficult to take into account when someone wants to fabricate data. As such, using these multivariate associations to detect anomalies from genuine data might prove valuable.

The multivariate associations between various variables can be estimated from control data that are (assumably) genuine. For example, the multivariate relation between the means (Ms) and standard deviations (SDs) can be collected from comparable studies and measures in the literature. It is important to collect information from homogeneous studies and measures, to limit the influence of confounders that either suppress or exacerbate the relation. If the study under investigation for example uses a Hot Sauce measure for aggression [@10.1002/SICI1098-2337199925:5<331::AID-AB2>3.0.CO;2-1], it is important to collect data on that Hot Sauce measure's properties in other studies that use it, not in studies that use various measures for aggression.

Subsequently, the (non-)parametric estimates of the multivariate relations can be used to determine how extreme the observed multivariate relations are. Consider the following example, with parametric estimates of the multivariate association between Ms and SDs. For example, imagine a set of studies that use the Hot Sauce aggression measure. When meta-analyzed, the Fisher transformed correlation [@10.2307/2331838] between the Ms and SDs across studies is estimated to be $0.123$, with a standard deviation of $0.1$. When an investigated paper presents results from a Hot Sauce aggression measure that correlate highly (Fisher transformed: $.5$), this indicates there might be an anomaly, considering only $`r pnorm(.5, .123, .1, lower.tail = FALSE)`$ of studies are expected to show such a strong relation (or more extreme) between Ms and SDs if the control data are representative of the population effect. 

Contrary to the aforementioned methods, this method is less generic because it requires the investigator to collect control data. However, because of this, it also has the potential to be more sensitive then the generic methods. Additionally, multivariate associations are always present in data and therefore the applicability of this method would be greater than the generic analyses that have a specific set of conditions that need to be fulfilled before the methods are applicable.

# Study 1

We tested the performance of statistical methods to detect data fabrication in summary results with genuine- and fabricated summary results from four anchoring studies [@10.1126/science.185.4157.1124;@10.1037/e722982011-058]. The anchoring effect is a well-known psychological heuristic that uses the information in the question as the starting point for the answer, which is then adjusted to yield a final estimate of a quantity. For example 'Is the percentage of African countries in the United Nations more or less than [10\% or 65\%]?'. These questions yield mean responses of 25\% and 45\%, respectively [@10.1126/science.185.4157.1124], despite essentially posing the same factual question. A considerable amount of genuine datasets on this heuristic are freely available and we collected fabricated datasets within this study. This study was approved by the Tilburg Ethical Review Board (EC-2015.50).

## Methods

```{r prep study 1, echo=FALSE}
# Study 1
# Compute the summary statistics for Many Labs
suppressWarnings(suppressMessages(source(sprintf('%sfunctions/compute_summary_anch_ml.R', x12))))
# Process the collected, fabricated data
suppressMessages(source(sprintf('%sfunctions/process_qualtrics_anch_01.R', x12)))
# Concatenate, analyze, and write out all ML and qualtrics data
# Uncomment this line to rerun, but take care: might take a while
if(!file.exists(sprintf('%sdata/study_01/study1_res.csv', x12)))
{
  set.seed(123);suppressMessages(source(sprintf('%sfunctions/concatenate_analyze_01.R', x12)))  
}
```

We collected summary results for four anchoring studies: (i) distance from San Francisco to New York, (ii) population of Chicago, (iii) height of the Mount Everest, and (iv) the number of babies born per day in the United States [@10.1037/e722982011-058]. Each of the four studies provided us with summary results for a 2 (low/high anchoring) $\times$ 2 (male/female) factorial design. Throughout this study, the unit of analysis is a set of summary statistics (i.e., means, standard deviations, and test results) for the four anchoring studies from one respondent. For current purposes, a respondent is defined as researcher/lab where the four anchoring studies' summary statistics originate from. All materials, data, and analyses scripts are freely available on the OSF ([https://osf.io/b24pq](https://osf.io/b24pq)) and a preregistration is available at [https://osf.io/ejf5x](https://osf.io/ejf5x) (deviations are explicated in this report).

### Data collection

We downloaded thirty-six genuine datasets from the publicly available Many Labs (ML) project [[https://osf.io/pqf9r](https://osf.io/pqf9r); @10.1027/1864-9335/a000178]. The ML project replicated several effects across thirty-six locations, including the anchoring effect in the four studies mentioned previously. Considering the size of the ML project, the transparency of research results, and minimal individual gain for fabricating data, we assumed these data to be genuine. For each of the thirty-six locations we computed sample sizes, means, and standard deviations for each of the four conditions in the four anchoring studies (i.e., $3\times4\times4$) for each of the thirty-six locations. We computed these summary statistics from the raw ML data, which were cleaned using the original analysis scripts from the ML project.

Using quotum sampling, we collected thirty-six fabricated datasets of summary results for the same four anchoring studies. Quotum sampling was used to sample as many responses as possible for the available 36 rewards (i.e., not all respondents might request the gift card and count towards the quotum; one participant did not request a reward). The sampling frame consisted of 2,038 psychology researchers who published a peer-reviewed paper in 2015, as indexed in Web of Science (WoS) with the filter set to the U.S. We sampled psychology researchers to improve familiarity with the anchoring effect [@10.1126/science.185.4157.1124;@10.1037/e722982011-058], for which summary results were fabricated. We filtered for U.S. researchers to ensure familiarity with the imperial measurement system, which is the scale of some of the anchoring studies (note: we found out several non-U.S. researchers were included because the WoS filter also retained papers with co-authors from the U.S.). WoS was searched on October 13, 2015. In total, 2,038 unique corresponding e-mails were extracted from 2,014 papers (due to multiple corresponding authors).

We invited a random sample of 1,000 researchers via e-mail to participate in this study on April 25, 2016 (invitation: [https://osf.io/s4w8r](https://osf.io/s4w8r)). The study took place via Qualtrics with anonimization procedures in place (e.g., no IP-addresses saved). We informed the participating researchers that the study would require them to fabricate data and explicitly mentioned that we would investigate these data with statistical methods to detect data fabrication. We also clarified to the  respondents that they could stop at any time without providing a reason. If they wanted, respondents received a $30 Amazon gift card as compensation for their participation if they were willing to enter their email address. They could win an additional $50 Amazon gift card if they were one of three top fabricators. The provided e-mail addresses were unlinked from individual responses upon sending the bonus gift cards. The full text of the Qualtrics survey is available at [https://osf.io/w984b](https://osf.io/w984b).

Each respondent was instructed to fabricate 32 summary statistics (4 studies $\times$ 2 conditions $\times$ 2 sexes $\times$ 2 statistics [mean and sd]) that fulfilled three hypotheses. We instructed respondents to fabricate results for the following hypotheses: there is (i) a main effect of condition, (ii) no effect of sex, and (iii) no interaction effect between condition and sex. We fixed the sample sizes to 25 per cell; respondents did not need to fabricate sample sizes. The fabricated summary statistics and their accompanying test results for these three hypotheses serve as the data to examine the properties of statistical tools to detect data fabrication.

We provided respondents with a template spreadsheet to fill out the fabricated data, in order to standardize the fabrication process without restraining the participant in how they chose to fabricate data. Figure `r fignr` depicts an example of this spreadsheet (original:  [https://osf.io/w6v4u](https://osf.io/w6v4u)). We requested respondents to fill in the yellow cells with fabricated data, which includes means and the standard deviations for four conditions. Using these values, statistical tests are computed and shown in the "Current result" column instantaneously. If these results confirmed the hypotheses, a checkmark appeared as depicted in Figure `r fignr`. We required respondents to copy-paste the yellow cells into Qualtrics, to provide a standardized response format that could be automatically processed in the analyses.

```{r spreadsheet-study1, fig.cap="Example of a filled in template spreadsheet used in the fabrication process of Study 1. Respondents fabricated data in the yellow cells, which were used to compute the results of the hypothesis tests. If the fabricated data confirm the hypotheses, a checkmark appeared in a green cell (one of four template spreadsheets available at [https://osf.io/w6v4u/](https://osf.io/w6v4u/)).", out.width='100%', echo=FALSE}
knitr::include_graphics('../figures/spreadsheet.png')

fignr <- fignr + 1
```

Upon completing the fabrication of the data, respondents were debriefed. Respondents answered several questions about their statistical knowledge and approach to data fabrication and finally we reminded them that data fabrication is widely condemned by professional organizations, institutions, and funding agencies alike. We rewarded participation with a $30 Amazon gift card and the fabricated results that were most difficult to detect received a bonus $50 Amazon gift card.

### Data analysis

```{r analysis study 1, echo=FALSE}
# Read in datafile, just in case the computations were not re-run.
dat_summary <- read.csv(dat_summary_file)
# Select only those from qualtrics who did for bonus
# removed <- sum(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus))
# dat_summary <- dat_summary[!(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus)), ]

type_index <- which(names(dat_summary) == 'type')
temp <- dat_summary[, c(type_index, 18:length(names(dat_summary)))]  
dat_summary$es_gender <- sqrt(dat_summary$es2_gender)
dat_summary$es_condition <- sqrt(dat_summary$es2_condition)
dat_summary$es_interaction <- sqrt(dat_summary$es2_interaction)

# naming just for ease of use later
dat_summary$es_p <- 1 - dat_summary$es_condition
```

We analyzed the genuine- and fabricated datasets for the four anchoring studies in four ways. First, we applied variance analyses to the reported variances of each of the four groups per study separately. Second, we applied the reversed Fisher method to the results of the gender and interaction hypotheses (i.e., nonsignificant results) across the four studies. Third, we combined the results from the variance analyses and the reversed Fisher method, using the original Fisher method [@fisher1925]. Fourth, and not preregistered, we used effect size analysis (i.e., $1-r$) that is a proxy of how extreme an effect is.

Specifically for the variance analyses, we deviated from the preregistration. Initially, we simultaneously analyzed the reported variances per study across the anchoring conditions. However, upon analyzing these values, we realized that the variance analyses assume that the reported variances are from the same population distribution, which is not necessarily the case for the anchoring conditions. Hence, we included two variance analyses per anchoring study (i.e., one for the high anchoring condition and one for the low anchoring condition). In the results we differentiate between these by using 'homogeneous' (across conditions) and 'heterogeneous' (separated for low- and high anchoring conditions).

For each of these statistical tests to detect data fabrication we carried out sensitivity and specificity analyses using Area Under Receiving Operator Characteristic (AUROC) curves. AUROC-analyses indicate the sensitivity (i.e., True Positive Rate [TPR]) and specificity (i.e., True Negative Rate [TNR]) for various decision criteria (e.g., $\alpha=0, .01, .02, ..., .99, 1$). With these AUROC-curves, informed decisions about optimal alpha levels can be made based on various criteria. In this case, we determine the optimal alpha level by finding that alpha level for which the combination of TPR and TNR were highest. For example, if $\alpha=.04$ results in $TPR=.30$ and $TNR=.70$, but $\alpha=.05$ results in $TPR=.5$ and $TNR=.5$, .05 was chosen as an optimal decision criterion based on the sample. 

AUROC values indicates the probability that a randomly drawn fabricated- and genuine dataset can be correctly classified as fabricated and genuine [@10.1148/radiology.143.1.7063747]. In other words, if $AUROC=.5$, correctly classifying a randomly drawn dataset in this sample is equal to a coin flip. For this setting, we will regard any AUROC $<.6$ as plainly insufficient for detecting data fabrication, $.6\leq$ AUROC $<.7$ as failed, $.7\leq$ AUROC $<.8$ as sufficient, $.8\leq$ AUROC $<.9$ as good, and $.9\leq$ AUROC $\leq 1$ as excellent.

## Results

```{r echo=FALSE}
tp <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fp <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fn <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
tn <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

temp <- names(dat_summary)[grepl(names(dat_summary), pattern = "_p")]

j <- 1

for (variables in which(grepl(names(dat_summary), pattern = "_p")))
{
  i <- 1 
  
  for (threshold in seq(0, 1, .01))
  {
    tp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    fp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'ml', na.rm = TRUE)
    fn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    tn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'ml', na.rm = TRUE)
    
    i <- i + 1
  }
  
  j <- j + 1
}

# loop through matrix for proper comps

tpr <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fpr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fnr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
tnr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

ppv <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fdr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
npv <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
false_or <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

for (i in 1:sum(grepl(names(dat_summary), pattern = "_p")))
{
  tpr[, i] <- tp[, i] / (tp[, i] + fn[, i])
  fnr[, i] <- fn[, i] / (tp[, i] + fn[, i])
  fpr[, i] <- fp[, i] / (fp[, i] + tn[, i])
  tnr[, i] <- tn[, i] / (fp[, i] + tn[, i])
  
  ppv[, i] <- tp[, i] / (fp[, i] + tp[, i])
  fdr[, i] <- fp[, i] / (fp[, i] + tp[, i])
  npv[, i] <- tn[, i] / (tn[, i] + fn[, i])
  false_or[, i] <- fn[, i] / (tn[, i] + fn[, i])
}
fpr <- as.data.frame(fpr)
tpr <- as.data.frame(tpr)
fnr <- as.data.frame(fnr)
tnr <- as.data.frame(tnr)

ppv <- as.data.frame(ppv)
fdr <- as.data.frame(fdr)
npv <- as.data.frame(npv)
false_or <- as.data.frame(false_or)

names(fpr) <- temp
names(tpr) <- temp
names(fnr) <- temp
names(tnr) <- temp

names(ppv) <- temp
names(fdr) <- temp
names(npv) <- temp
names(false_or) <- temp


height = (tpr[-1, ] + tpr[-dim(tpr)[1], ])/2
width = apply(fpr, 2, diff) # = diff(rev(omspec))
AUC_res <- apply(height * width, 2, sum)

AUC <- as.numeric(AUC_res)

temp <- names(dat_summary)[grepl(names(dat_summary), pattern = "_p")]
Method <- c("$SD_z$ homogeneous all",
            "$SD_z$ study 1",
            "$SD_z$ study 2",
            "$SD_z$ study 3",
            "$SD_z$ study 4",
            "$maxmin_z$ homogeneous all",
            "$maxmin_z$ study 1",
            "$maxmin_z$ study 2",
            "$maxmin_z$ study 3",
            "$maxmin_z$ study 4",
            "$SD_z$ heterogeneous all",
            "$SD_z$ study 1 low condition",
            "$SD_z$ study 1 high condition",
            "$SD_z$ study 2 low condition",
            "$SD_z$ study 2 high condition",
            "$SD_z$ study 3 low condition",
            "$SD_z$ study 3 high condition",
            "$SD_z$ study 4 low condition",
            "$SD_z$ study 4 high condition",
            "$maxmin_z$ heterogeneous all",
            "$maxmin_z$ study 1 low condition",
            "$maxmin_z$ study 1 high condition",
            "$maxmin_z$ study 2 low condition",
            "$maxmin_z$ study 2 high condition",
            "$maxmin_z$ study 3 low condition",
            "$maxmin_z$ study 3 high condition",
            "$maxmin_z$ study 4 low condition",
            "$maxmin_z$ study 4 high condition",
            "Reversed Fisher method gender hypothesis",
            "Reversed Fisher method interaction hypothesis",
            "Combined Fisher test (3 results, $SD_z$ homogeneous)",
            "Combined Fisher test (3 results, $SD_z$ heterogeneous)",
            "Combined Fisher test (6 results, $SD_z$ homogeneous)",
            "Combined Fisher test (10 results, $SD_z$ heterogeneous)",
            "Effect sizes ($1-r$)")

df <- data.frame(Method = Method,
                 AUROC = AUC_res)
```

The collected data included `r table(dat_summary$type)[1]` genuine data from Many Labs 1 [[https://osf.io/pqf9r](https://osf.io/pqf9r); @10.1027/1864-9335/a000178] and `r table(dat_summary$type)[2]` fabricated datasets ([https://osf.io/e6zys](https://osf.io/e6zys); `r sum(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus))` participants did not participate for a bonus). 

Figure `r fignr` shows a group-level comparison of the genuine- and fabricated $p$-values and effect sizes ($r$). These group-level comparisons provide an overview of the differences between the genuine- and fabricated data [see also @10.1186/1471-2288-3-18]. These distributions indicate little group differences between genuine- and fabricated data when nonsignificant effects are inspected (i.e., gender and interaction hypotheses). However, there seem to be large group differences when we required subjects to fabricate significant data (i.e., condition hypothesis). Considering this, we also investigated how well effect sizes perform in detecting data fabrication (not preregistered). In the following sections, we investigate the performance of such statistical methods to detect data fabrication on an respondent-level basis.

```{r ddfab_density, fig.cap="Overlay of density distributions for both genuine and fabricated data, per effect and type of result. We instructed respondents to fabricate nonsignificant data for the gender and interaction effects, and a significant effect for the condition effect.", out.width='100%', echo=FALSE}
plot_p_gender <- ggplot(dat_summary, aes(x = p_gender, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("$P$-value")) + 
    ylab("Density") + 
    ylim(0, 1.5) + 
    scale_fill_discrete(guide=FALSE) + 
    ggtitle('Gender')
  
  plot_es_gender <- ggplot(dat_summary, aes(x = es_gender, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  # Condition
  dat_summary_tmp <- dat_summary
  dat_summary_tmp$p_condition <- log10(dat_summary_tmp$p_condition)
  plot_p_condition <- ggplot(dat_summary_tmp, aes(x = p_condition, fill = type)) +
    geom_density(alpha = .3) +
    xlab(TeX("log10 $P$-value")) +
    ylab("Density") +
    scale_fill_discrete(labels = c("Genuine","Fabricated")) + 
    theme(legend.position="top") +
    ggtitle('Condition')
  
  plot_p_condition$labels$fill <- ""
  
  plot_es_condition <- ggplot(dat_summary, aes(x = es_condition, fill = type)) + 
    geom_density(alpha = .3) + 
    xlim(0, 1) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  
  # Interaction
  plot_p_interaction <- ggplot(dat_summary, aes(x = p_interaction, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("$P$-value")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE) +
    ylim(0, 1.5) +
    ggtitle('Interaction')
  
  plot_es_interaction <- ggplot(dat_summary, aes(x = es_interaction, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  # Plot
  grid.arrange(plot_p_gender,
               plot_p_condition,
               plot_p_interaction,
               plot_es_gender,
               plot_es_condition,
               plot_es_interaction,
               ncol = 3)

fignr <- fignr + 1
```

### Performance of variance analysis to detect data fabrication

Table `r tabnr` indicates that both operationalizations (i.e., $SD_z$ and $max-min_z$) show similar performance based on the AUROC. All in all, their performance ranges from `r round(min(df$AUROC[1:28]), 3)` through `r round(max(df$AUROC[1:28]), 3)`. As such, there is considerable variation for the various applications of the variance analyses.

```{r table_var, echo=FALSE}
dfvar <- data.frame(Method = c("Homogeneous, all studies combined",
               "Homogeneous, study 1",
               "Homogeneous, study 2",
               "Homogeneous, study 3",
               "Homogeneous, study 4",
               "Heterogeneous, all studies combined",
               "Heterogeneous study 1, low anchor condition",
               "Heterogeneous study 1, high anchor condition",
               "Heterogeneous study 2, low anchor condition",
               "Heterogeneous study 2, high anchor condition",
               "Heterogeneous study 3, low anchor condition",
               "Heterogeneous study 3, high anchor condition",
               "Heterogeneous study 4, low anchor condition",
               "Heterogeneous study 4, high anchor condition"),
               SD_z = df$AUROC[c(1:5, 11:19)],
               max_min_z = df$AUROC[c(6:10, 20:28)])
dfvar2 <- dfvar
names(dfvar2) <- c('Method', 'AUROC $SD_z$', 'AUROC $max-min_z$')
knitr::kable(dfvar2, row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using variance analyses to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

Combining the variance analyses across the different studies improves performance. This is as expected, considering that the sample size increases for the analyses (i.e., more reported variances are included) and that causes an increase in the statistical power to detect data fabrication. 

More notably, combining the studies and taking the heterogeneous approach (i.e., separating anchoring conditions) greatly increases the performance to detect data fabrication considerably. Where the AUROC under homogeneous variances for $SD_z=`r round(dfvar$SD_z[1], 3)`$ ($max-min_z=`r round(dfvar$max_min_z[1], 3)`$), under heterogeneous variances it increases to $SD_z=`r round(dfvar$SD_z[6], 3)`$ ($max-min_z=`r round(dfvar$max_min_z[6], 3)`$). Further inspecting the heterogeneous variance of variances analysis indicates that no false positives occur until $\alpha=`r seq(0, 1, .01)[max(which(tnr$variance_sd_p_overall_hetero == 1))]`$, making this the optimal alpha level based on this sample (but note the small sample). 

### Performance of $p$-values analysis to detect data fabrication

Table `r tabnr` indicates that methods using nonsignificant $p$-values to detect data fabrication are hardly better than chance level in the current sample. We asked researchers to fabricate data for nonsignificant effect sizes, thinking they might be unable to produce uniformly distributed $p$-values. However, these results (and the density plot in Figure XX) indicate that widespread detection based on this is not promising.

```{r table_p, echo=FALSE}
knitr::kable(df[29:30,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using $p$-value analyses to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of combining variance- and $p$-value analyis to detect data fabrication

Table `r tabnr` indicates that combining the variance- and $p$-value methods provides little beyond the methods separately. The overall performance of this combination is driven by the variance analyses, given that the $p$-value analysis yields little more than chance classification. When combining the results from variance analyses per anchoring condition (i.e., 10 results, $SD_z$ heterogeneous) and the $p$-value analyses, a minor improvement occurs over the heterogeneous variance analysis (all studies combined, $AUROC=`r round(dfvar$SD_z[6], 3)`$). However, this difference is negligible and potentially due to sampling error. 

```{r table_comb, echo=FALSE}
knitr::kable(df[31:34,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of combining variance- and $p$-value analyses to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

### Performance of extreme effects to detect data fabrication

Table `r tabnr` indicates that using effect sizes (i.e., $1-r$) is a simple but effective way to detect data fabrication ($AUROC=`r round(df[35,2], 3)`$). Compared to the variance analyses several sections ago, its performance in this sample is a bit worse (i.e., `r round(df[35,2], 3)` compared to `r round(dfvar$SD_z[6], 3)`). However, it makes up for this by computational parsimony. Whereas the variance analyses require a considerable amount of effort to implement, computing the correlation and taking the inverse is a relatively simple task. Further inspecting the effect size approach to detecting data fabrication indicates that no false positives occur until  $\alpha=`r seq(0, 1, .01)[max(which(tnr$es_p == 1))]`$ (i.e., $r>`r seq(1, 0, -.01)[max(which(tnr$es_p == 1))]`$), making this the optimal alpha level based on this sample (but note the small sample).

```{r table_es, echo=FALSE}
knitr::kable(df[35,], row.names = FALSE, digits = 3, caption = "_Table XX._ Diagnosticity of using effect sizes to detect data fabrication, depicted with the AUROC-value.")
tabnr <- tabnr + 1
```

<!-- ## Discussion -->

<!-- These data corroborate the idea that extremely large effect sizes (e.g., $r>.95$) might prove to be an easy-to-implement flag for potentially anomalous data (it is wise to seek for alternative explanations after flagging, however).  -->

<!-- For the anchoring studies, homogeneous population variances across conditions seem unreasonable and greatly affect the performance of the statistical method. These result highlight that the origin of the variances should be  -->

<!-- % variance analysis and effect size best ways to inspect
% but severely correlated: smaller variances = larger effects
% easiest way is too look for massive effects in the effects

% Simonsohn method not invalid under homogeneous, just somethign to take into account
% easily adjusted

% Discuss how the PPV is likely to be overestimated because prevalence is 50% in this study
% How we don't know the prevalence, in fact.

% Discuss alpha choice based on sample has large error (small sample).
% Only provides a first indication

% Comparison between methods not tested, large error given small sample.
% This provides just initial work -->

<!-- This could be done automatically at a large scale easily [data for this are available in @10.3758/s13428-015-0664-2;@10.3390/data1030014] or by a reader while going through a manuscript (e.g., during peer-review). wrt effect sizes -->


# Study 2

```{r prep study 2, echo=FALSE}
ml3_dat_file <- sprintf('%sdata/study_02/study_02-ml3_stroop/StroopCleanSet.csv', x12)
pdf_file <- sprintf('%sarchive/gender_interaction.pdf', x12)

# Set the number of iterations to use in calculations
iter <- 100000

# Compute the processed raw data for Many Labs 3 stroop
if(!file.exists(sprintf('%sdata/study_02/ml3_stroop.csv', x12)))
{
  source(sprintf('%sfunctions/compute_raw_stroop_ml3.R', x12))
}
```

We investigated detecting data fabrication in raw data as an extension of Study 1. In essence, the procedure is similar: we asked actual researchers to fabricate data that they thought would go undetected. For Study 2 we included a face-to-face interview to qualitatively assess how data fabrication occurs. A preregistration of this study occurred during the seeking of funding [@10.3897/rio.2.e8860] and during data collection ([https://osf.io/fc35g](https://osf.io/fc35g)).

To test the validity of statistical methods to detect data fabrication in raw data, we investigated raw data of a Stroop experiment [@10.1037/h0054651]. In the Stroop task, participants are asked to determine the color a word is presented in (i.e., word colors), but the word also reads a color (i.e., color words). The presented word color (i.e., 'red', 'blue', or 'green') can be either presented in the congruent color (e.g., 'red' presented in red) or an incongruent color (i.e., 'red' presented in green). The dependent variable in the Stroop task is the response latency (in this study milliseconds are used). Participants in actual studies are typically presented with a set of these, where the mean and standard deviation per condition serves as the raw data. The Stroop effect typically is computed as the difference in mean response latencies between the congruent and incongruent conditions.

## Methods

### Data collection

Twenty-one genuine datasets on the Stroop task were collected from the Many Labs 3 project [[https://osf.io/n8xa7/](https://osf.io/n8xa7/); @10.1016/j.jesp.2015.10.012]. Many Labs 3 (ML3) includes 20 participant pools from universities and one online sample [the original preregistration mentioned 20 datasets, accidentally overlooking the online sample; @10.3897/rio.2.e8860]. Using the original raw data and analysis script from ML3 ([https://osf.io/qs8tp/](https://osf.io/qs8tp/)), we computed the mean (M) and standard deviation (SD) for the participant's response latencies in both the within-subjects conditions of congruent trials and incongruent trials. These also formed the basis for the template of the data that needed to be fabricated by the participants (see also Figure X). The Stroop effect was calculated as a $t$-test of the difference ($H_0:\mu=0$).

```{r spreadsheet-study2, fig.cap="Example of a filled in template spreadsheet used in the fabrication process for Study 2. Respondents fabricated data in the yellow cells and green cells, which were used to compute the results of the hypothesis test of the condition effect. If the fabricated data confirm the hypotheses, a checkmark appeared. This template is available at [https://osf.io/2qrbs/](https://osf.io/2qrbs/).", out.width='100%', echo=FALSE}
knitr::include_graphics('../figures/spreadsheet2.png')
```

We collected twenty-eight faked datasets on the Stroop task experimentally in a two-stage sampling procedure. First, we invited 80 Dutch and Flemish psychology researchers who published a peer-reviewed paper on the Stroop task between 2005-2015 as available in the Thomson Reuters’ Web of Science database. We selected Dutch and Flemish researchers to allow for a face-to-face interview on how the data were fabricated. We chose the period 2005-2015 to prevent a drastic decrease in the probability that the corresponding author would still be addressable via the given email. The database was searched on October 10, 2016 and 80 unique e-mails were retrieved from 90 publications. Only two of these 80 participated in the study; we subsequently implemented a second sampling stage where we collected e-mails from all PhD-candidates, teachers, and professors of psychology related departments at Dutch universities. This resulted in 1659 additional unique e-mails that we subsequently invited to participate in this study. Due to a malfunction in Qualtrics' quotum sampling, we oversampled, resulting in 28 participants instead of the originally intended 20 participants.

Each participant received instructions on the data fabrication task via Qualtrics but was allowed to fabricate data until the face-to-face interview took place. In other words, each participant could take the time they wanted/needed to fabricate the data as extensively as they liked. Each participant received downloadable instructions (original: [https://osf.io/7qhy8/](https://osf.io/7qhy8/)) and the template spreadsheet via Qualtrics (see Figure X; [https://osf.io/2qrbs/](https://osf.io/2qrbs/)). The interview was scheduled via Qualtrics with JGV, who blinded the rest of the research team from the identifying information of each participant and the date of the interview. All interviews took place between January 31 and March 3, 2017. To incentivize researchers to participate, they received 100 euros for participation; to incentivize them to fabricate (supposedly) hard to detect data they could win an additional 100 euros if they belonged to one out of three top fabricators. The contents of the interview were transcribed for further research on qualitatively assessing how researchers might fabricate experimental data.

### Data analysis

```{r ml3}
ml3_dat <- read.csv(sprintf('%sdata/study_02/ml3_stroop.csv', x12))

df_ml3 <- NULL
# Compute the results for the ML3 data
for(i in 1:length(unique(ml3_dat$study_name)))
{
  tmp <- ml3_dat[ml3_dat$study_name == unique(ml3_dat$study_name)[i], ]
  # This is an object to simplify concatenation
  binder <- data.frame(id = unique(ml3_dat$study_name)[i],
                       benford_congr_m_p = digit_analysis(tmp$MC, type = 'benford')$pval,
                       benford_congr_sd_p = digit_analysis(tmp$SDC, type = 'benford')$pval,
                       benford_incongr_m_p = digit_analysis(tmp$MI, type = 'benford')$pval,
                       benford_incongr_sd_p = digit_analysis(tmp$SDI, type = 'benford')$pval,
                       terminal_congr_m_p = digit_analysis(tmp$MC, type = 'terminal')$pval,
                       terminal_congr_sd_p = digit_analysis(tmp$SDC, type = 'terminal')$pval,
                       terminal_incongr_m_p = digit_analysis(tmp$MI, type = 'terminal')$pval,
                       terminal_incongr_sd_p = digit_analysis(tmp$SDI, type = 'terminal')$pval,
                       std_congr_p = std_var(tmp$SDC, tmp$NC, iter = iter),
                       std_incongr_p = std_var(tmp$SDI, tmp$NI, iter = iter),
                       mult_m_sd_congr = cor(tmp$MC, tmp$SDC),
                       mult_m_sd_incongr = cor(tmp$MI, tmp$SDI),
                       mult_m_m_across = cor(tmp$MC, tmp$MI),
                       mult_sd_sd_across = cor(tmp$SDC, tmp$SDI))
  
  df_ml3 <- rbind(df_ml3, binder)
}

# The responses from the study
responses <- list.files(sprintf('%sdata/study_02/responses', x12))
df_fab <- NULL

for (response in responses)
{
  fab_dat <- read.table(sprintf('%sdata/study_02/responses/%s', x12, response),
                        sep = '\t', header = TRUE)
  # For clarity sake
  names(fab_dat) <- c('id',
                      'mean_congruent',
                      'sd_congruent',
                      'congruent_trials',
                      'mean_incongruent',
                      'sd_incongruent',
                      'incongruent_trials')
  
  mult_m_sd_congr <- cor(fab_dat$mean_congruent, fab_dat$sd_congruent)
  mult_m_sd_incongr <- cor(fab_dat$mean_incongruent, fab_dat$sd_incongruent)
  mult_m_m_across <- cor(fab_dat$mean_congruent, fab_dat$mean_incongruent)
  mult_sd_sd_across <- cor(fab_dat$sd_congruent, fab_dat$sd_incongruent)
  
  # Slotted for removal and replacement by parametric approach 20170628
  p_mult_m_sd_congr <- ifelse(mean(df_ml3$mult_m_sd_congr) > mult_m_sd_congr,
                              sum(df_ml3$mult_m_sd_congr < mult_m_sd_congr) * 2 / length(df_ml3$mult_m_sd_congr),
                              sum(df_ml3$mult_m_sd_congr > mult_m_sd_congr) * 2 / length(df_ml3$mult_m_sd_congr))
  p_mult_m_sd_incongr <- ifelse(mean(df_ml3$mult_m_sd_incongr) > mult_m_sd_incongr,
                                sum(df_ml3$mult_m_sd_incongr < mult_m_sd_incongr) * 2 / length(df_ml3$mult_m_sd_incongr),
                                sum(df_ml3$mult_m_sd_incongr > mult_m_sd_incongr) * 2 / length(df_ml3$mult_m_sd_incongr))
  p_mult_m_m_across <- ifelse(mean(df_ml3$mult_m_m_across) > mult_m_m_across,
                              sum(df_ml3$mult_m_m_across < mult_m_m_across) * 2 / length(df_ml3$mult_m_m_across),
                              sum(df_ml3$mult_m_m_across > mult_m_m_across) * 2 / length(df_ml3$mult_m_m_across))
  p_mult_sd_sd_across <- ifelse(mean(df_ml3$mult_sd_sd_across) > mult_sd_sd_across,
                                sum(df_ml3$mult_sd_sd_across < mult_sd_sd_across) * 2 / length(df_ml3$mult_sd_sd_across),
                                sum(df_ml3$mult_sd_sd_across > mult_sd_sd_across) * 2 / length(df_ml3$mult_sd_sd_across))
  
  binder <- data.frame(id = response,
                       benford_congr_m_p = digit_analysis(fab_dat$mean_congruent, type = 'benford')$pval,
                       benford_congr_sd_p = digit_analysis(fab_dat$sd_congruent, type = 'benford')$pval,
                       benford_incongr_m_p = digit_analysis(fab_dat$mean_incongruent, type = 'benford')$pval,
                       benford_incongr_sd_p = digit_analysis(fab_dat$sd_incongruent, type = 'benford')$pval,
                       terminal_congr_m_p = digit_analysis(fab_dat$mean_congruent, type = 'terminal')$pval,
                       terminal_congr_sd_p = digit_analysis(fab_dat$sd_congruent, type = 'terminal')$pval,
                       terminal_incongr_m_p = digit_analysis(fab_dat$mean_incongruent, type = 'terminal')$pval,
                       terminal_incongr_sd_p = digit_analysis(fab_dat$sd_incongruent, type = 'terminal')$pval,
                       std_congr_p = std_var(fab_dat$sd_congruent, fab_dat$congruent_trials, iter = iter),
                       std_incongr_p = std_var(fab_dat$sd_incongruent, fab_dat$incongruent_trials, iter = iter),
                       p_mult_m_sd_congr,
                       p_mult_m_sd_incongr,
                       p_mult_m_m_across,
                       p_mult_sd_sd_across)
  
  # Add 1 ten millionth to any zero
  # so the log doesn't go to infinite and messes with the Fisher method
  p_for_fish <- unlist(ifelse(binder[,6:15] == 0, .0000001, binder[,6:15]))
  
  fish <- -2 * sum(log(p_for_fish))
  df_fish <- length(p_for_fish) * 2
  p_fish <- pchisq(q = fish, df = df_fish, lower.tail = FALSE)
  binder <- cbind(binder, p_fish)
  
  df_fab <- rbind(df_fab, binder)
}

write.csv(df_fab, sprintf('%sdata/study_02/df_fab.csv', x12), row.names = FALSE)
```

## Results

## Discussion

# General discussion

## `ddfab` package

All the methods used in this paper are also implemented in the open source R package `ddfab`.

However, these results indicate that application of these methods should be informed and as a screening method.

## Study fallout

While conducting Study 2 reported in this paper, there was considerable criticism from several parties.

We 

# Session info

```{r}
sessionInfo()
```

# References
