---
title: "Ecological validity of detecting data fabrication in experimental studies."
author: "Chris HJ Hartgerink, Jelte M Wicherts, Marcel ALM van Assen"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
csl: ../bibliography/apa.csl
bibliography: ../bibliography/library.bib
---

```{r prepwork, echo = FALSE}
suppressPackageStartupMessages(if(!require(pROC)){install.packages('pROC')})
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(if(!require(foreign)){install.packages('foreign')})
suppressPackageStartupMessages(library(foreign))

# Add ../ before running rmarkdown, or vice versa when not running there
x12 <- '../'
# x12 <- ''
```


<!---

# Abstract
--placeholder--

# Introduction

Data fabrication is malicious to scientific knowledge, hence it is imperative to be able to detect it. An estimated 2% of all researchers admit to having fabricated data at least once, and 14% have said to have witnessed someone fabricate data [@fanelli2009]. In stark contrast to plagiarism, which has a similar admittance rate but double the witness rate [@pupovac2015], hardly any systems are implemented during the editorial process to detect problematic data. Recently, one journal has explicitly stated they are considering such a system (Miller 2015), another is looking BMC SCIENCEMATTERS. Editors and reviewers hardly pay attention to potential problems with the data during the editor process either (Bornmann et al. 2008), which highlights the potential of automated detection systems.



Similar to how plagiarism scanners are implemented to detect plagiarism, statistical methods can be applied to detect potentially fabricated data. By extracting information from research papers, called content mining (Smith-Unna and Murray-Rust 2014), statistical analyses of the presented research results can be facilitated. Previous statistical content mining methods, extracting for example reported test results across 30,000 papers (Nuijten et al. 2015), indicate the feasibility of extracting statistical information from research papers in large quantities.

However helpful these technological advances are, the statistical methods applied to extracted data in order to detect potential data fabrication require thorough validation. More specifically, the statistical methods need to have a well-balanced trade-off between the false positive rate and the false negative rate. Considering the, presumably, low prevalence rate of data fabrication, false positive rates can have tremendous effects when applied at a large scale. For example, if we assume that 2% of the research papers is fabricated and we can detect it perfectly if it occurs, we would detect approximately 2.5 times more false positives than true positives. And this is if we assume the false positive rate is 5% and no higher.  

How data is fabricated by researchers is not well-known and simulating the properties of statistical methods to detect data fabrication is therefore invalid. When the process is unknown, it is hardly feasible to properly approximate reality and draw valid conclusions. Moreover, it seems reasonable to state that there are many different ways to fabricate data and that these different data fabrication ‘styles’ can be combined. Even if certain fabrication processes were known, simulating ecologically valid processes would still be quite difficult.

Nonetheless, statistical methods to detect data fabrication have been developed based on theoretical principles, which help detect data fabrication in experiments. These methods are based on the tenet that humans are bad at fabricating results that originate from probabilistic processes. Outcome variables that are inspected in experiments are based on a sample which results in observations that are the result of a random variable, hence subject to probabilistic (i.e., stochastic) processes. The inadequacy of humans in generating such sequences is the consequence of mental heuristics. For example, most humans succumb to the gambler’s fallacy, estimating the probability of heads as more likely when we have thrown tails several times (Tversky and Kahneman 1974). Other fallacious heuristics include the local-representativeness heuristic (), .

Statistical methods to detect data fabrication have been successfully applied in ad hoc investigations, but their value in screening processes remains opaque. For example, Simonsohn’s variance of variances method (Simonsohn 2013) has been applied to the Smeesters and Sanna cases, but only after suspicions had already arisen. The value of applying such a method as a screening measure is hard to assess regardless of its previous successes in ad hoc investigations.

In this paper, we experimentally test the validity of a set of statistical methods to detect data fabrication, based on summary results (Study 1) or raw data (Study 2), and assess the optimal balance between decision criterion and detection accuracy. We apply the statistical methods to detect data fabrication to both (assumed) genuine data from the Many Labs experiments and fabricated datasets, which we collected in an experimental setting where researchers were invited to fabricate data according to a controlled set of hypotheses. Both experiments were approved by the Tilburg Ethics committee (EC-2015.50).
--->
# Study 1

```{r, echo = FALSE}
# Dynamically generate path names
ml_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
qualtrics_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
dat_summary_file <- sprintf('%sdata/study_01/study1_res.csv', x12)
file <- sprintf('%sdata/study_01/raw_summary_results_fabrication_qualtrics.csv', x12)
res_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
ml_dat_file <- sprintf('%sdata/study_01/anchoring_ml/chjh ml1_anchoring cleaned.sav', x12)
summary_stat_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
pdf_file <- sprintf('%sarchive/gender_interaction.pdf', x12)

# Set the number of iterations to use in calculations
iter <- 100000

# Compute the summary statistics for Many Labs
source(sprintf('%sfunctions/compute_summary_anch_ml.R', x12))
# Process the collected, fabricated data
source(sprintf('%sfunctions/process_qualtrics_anch_01.R', x12))
# Concatenate, analyze, and write out all ML and qualtrics data
# Uncomment this line to rerun
# set.seed(123);source(sprintf('%sfunctions/concatenate_analyze_01.R', x12))
```


We tested the performance of statistical methods to detect data fabrication based on summary results with genuine and fabricated summary results of four anchoring studies [@tversky1974;@jacowitz1995]. The anchoring effect is a well-known psychological heuristic that uses the information in the question as the starting point for the answer, which is then adjusted to yield a final estimate of a quantity. For example 'Is the percentage of African countries in the United Nations more or less than [10\% or 65\%]?'. These questions yield mean responses of 25\% and 45\%, respectively [@tversky1974], despite essentially posing the same factual question. A considerable amount of genuine datasets on this heuristic are freely available and we collected fabricated datasets within this study.

## Methods

The four anchoring studies for which results were collected were (i) distance from San Francisco to New York, (ii) population of Chicago, (iii) height of the Mount Everest, and (iv) the number of babies born per day in the United States. Each of the four studies provided summary results for a 2 (low/high anchoring) × 2 (male/female) factorial design. Throughout this study, the unit of analysis is a set of summary statistics (i.e., means, standard deviations, and test results) for the four anchoring studies from one respondent. Respondent is defined as researcher/lab where the four anchoring studies' summary statistics originate from. All materials, data, and analyses scripts are freely available on the OSF ([osf.io/b24pq](osf.io/b24pq)) and were preregistered ([osf.io/ejf5x](osf.io/ejf5x); deviations are explicated in this report). 


### Data collection. 

We downloaded thirty-six genuine datasets from the publicly available Many Labs (ML) project [@klein2014, [osf.io/pqf9r](osf.io/pqf9r)]. The ML project replicated several effects across thirty-six locations, including the anchoring effect in the four studies mentioned previously. Considering the size of the ML project, the transparency of research results, and minimal individual gain for fraud, we assumed these data to be genuine. For each of the thirty-six locations, sample sizes, means, and standard deviations (four each) were computed, for each of the four conditions in the four anchoring studies across the thirty-six locations (i.e., $3\times4\times4\times36$). We computed these summary statistics from the raw ML data, which were cleaned using the original analysis scripts from the ML project.

Using quotum sampling, we collected thirty-six fabricated datasets of summary results for all four anchoring studies. The sampling frame consisted of 2,038 psychology researchers who published a peer-reviewed paper in 2015, as indexed in the Web of Science (WoS) with the filter set to the U.S. We sampled psychology researchers to improve familiarity with the anchoring effect [@jacowitz1995;@tversky1974], for which summary results were fabricated. We filtered for U.S. researchers to ensure familiarity with the imperial measurement system, which is the scale of some of the anchoring studies (note: we found out several non-U.S. researchers were included because this filter also retained papers with co-authors from the U.S.). WoS was searched on October 13, 2015. In total, 2,038 unique corresponding e-mails were extracted from 2,014 papers (due to multiple corresponding authors).

A random sample of 1,000 researchers were digitally approached to participate in this study on April 25, 2016 (invitation: [osf.io/s4w8r](osf.io/s4w8r)). The study took place via Qualtrics with anonimization procedures in place (e.g., no IP-addresses saved). We informed the participating researchers that the study would require them to fabricate data and explicitly mentioned that we would investigate these data with statistical methods to detect data fabrication. We also clarified to the  respondents that they could stop at any time without providing a reason. If they wanted, respondents received a $30 Amazon gift card as compensation for their participation for which they had to provide their email address, after which they could win an additional $50 Amazon gift card if they were one of three top fabricators. The provided email addresses were unlinked from individual responses upon sending the bonus gift cards. The text of the Qualtrics survey is available at [osf.io/w984b](osf.io/w984b).

Each respondent was instructed to fabricate 32 summary statistics (4 studies $\times$ 2 conditions $\times$ 2 sexes $\times$ 2 statistics [mean and sd]) that fulfilled three hypotheses. We instructed respondents to fabricate results for the hypotheses (i) main effect of condition, (ii) no effect of sex, and (iii) no interaction effect between condition and sex. Respondents did not need to fabricate sample sizes, which were set to 25 per cell a priori. The fabricated summary statistics and their accompanying test results for these three hypotheses serve as the data to examine the properties of tools to detect data fabrication.

We provided respondents with a template spreadsheet to fill out the fabricated data, in order to standardize the fabrication process without restraining the participant in how they choose to fabricate data. Figure `r fig <- 1; fig` depicts an example of this spreadsheet (original:  [osf.io/w6v4u](osf.io/w6v4u)). We requested the respondents to fill in the yellow cells with fabricated data, which includes means and the standard deviations for four conditions. Using these values, statistical tests are computed and shown in the "Current result" column instantaneously. When these results confirm the hypotheses, a checkmark appears as depicted in the green cells. We required respondents to copy-paste the yellow cells into Qualtrics, to provide a standardized response format that could be automatically processed in the analyses.

![spreadsheet](../figures/spreadsheet.png)

**Figure `r fig`.** Example of a filled in template spreadsheet used in the fabrication process. Respondents fabricated data in the yellow cells, which were used to compute the results of the hypothesis tests. If the fabricated data confirm the hypotheses, a checkmark appeared in a green cell.

Upon completing the fabrication of the data, respondents were debriefed. Several questions were asked about their statistical knowledge and approach to data fabrication. They were also reminded that data fabrication is widely condemned by professional organizations, institutions, and funding agencies alike.

We rewarded participation with a $30 Amazon gift card and the fabricated results that were most difficult to detect received a bonus $50 Amazon gift card. If the participant wanted to receive a compensation and contend for the bonus $50, he/she had to enter an email to receive the reward. These email addresses were unlinked from individual responses upon sending the gift cards. Quotum sampling was applied to sample as many responses as possible for the available 36 rewards (i.e., not all respondents might request the gift card and count towards the quotum; one participant did not request a reward).

### Data analysis.

To detect data fabrication in a set of summary results, we first test the variance of fabricated standard deviations [SDs; @simonsohn2013] across the four anchoring studies. This method tests whether the observed SDs contain a reasonable amount of variation, as expected based on stochastic sampling processes. For example, if four independent samples all yield the variance of 2.22, this could be considered excessively consistent when the probability that this amount of consistency (or more), given genuine samples, is less than 1 out of 1000 (this is simply an example). To compute this test, we first standardize the SDs for each of the four studies by computing

$\tilde{s}^2_j=\frac{s^2_j}{MS_w}=\frac{s^2_j}{\frac{\sum(N_j-1)s^2_j}{\sum(N_j-1)}}$
<!--- \limits^k_{j=1} --->

where $\tilde{s}_j$ denotes the standardized SD in group *j*. Note that $MS_w$ is the simple arithmetic mean when sample sizes are equal for all cells. We test several different measures to detect data fabrication that utilize these standardized SDs (i.e., $\tilde{s}_j$), including the max-min distance (denoted $\tilde{\sigma}_{max-min}$) and the variance of the standardized SDs [i.e.,  $\tilde{\sigma}_{sd}$; @simonsohn2013]. We compare the observed value for each $\tilde{\sigma}$ measure with the expected distribution of outcomes when the data would be the result of random sampling processes. To this end, we simulate the expected distribution of standardized SDs and compute the expected distribution of each $\tilde{\sigma}$. This expected distribution is used to determine the *p*-value of the observed $\tilde{\sigma}$ value. We simulate the standardized SD for each of the *j* groups as

$\tilde{s}^2_j\sim\Bigg[\frac{\chi^2_{N_j-1}}{N_j-1}\Bigg]/MS_w$

These simulated values are used to compute the expected distribution of all $\tilde{\sigma}$ measures.

Second, we apply the reversed Fisher method to the fabricated nonsignificant *p*-values twice, once for the gender effects hypothesis and once for the interaction effects hypothesis, in order to detect data fabrication. The Fisher method [@fisher1925] tests for evidence of an effect in a set of *p*-values and has previously been used as a meta-analytic method [@hong2008], but we adjust it here to test for results that are overly consistent with the null hypothesis. The original Fisher method is computed as

$\chi^2_{2k}=-2\sum\ln(p_i)$
<!--- \limits^k_{i=1} --->

and tests for right-skew in a set of *p*-values, but we adjust it to the following

$\chi^2_{2k}=-2\sum\ln(1-\frac{p_i-t}{1-t})$

where it now tests for left-skew (i.e., more larger *p*-values than smaller *p*-values) across the *k* number of *p*-values that falls above the threshold *t*. We set this threshold to .05 in order to include only nonsignificant test results. 

Finally, we combined the results of these three individual tests for data fabrication using the Fisher method. We expect this combination test of the three individual tests for data fabrication to be a more powerful than the individual tests. Based on the results of the combined test results, the three 'best' data fabricators are selected. The three respondents with the highest *p*-values contain the least evidential value for deviating from genuine data and receive an additional $50 Amazon gift card.

For each of these three tests to detect data fabrication we carry out sensitivity and specificity analyses using ROC-curves in order to determine optimal alpha levels. This analysis helps determine the classification performance of these three tests. In order to determine the optimal alpha level, we varied the alpha level from .000001 through .1 and assessed the classification performance of these methods to detect data fabrication. The optimal alpha level is determined by finding that alpha level for which both the true positive classification rate and the true negative classification rate are highest. For example,  


<!---
Something about reporting tendencies?
--->

### Results.
```{r inspect results to detect data fabrication, echo = FALSE}
# Read in datafile, just in case the computations were not re-run.
dat_summary <- read.csv(dat_summary_file)
dat_summary <- dat_summary[!(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus)), ]
# Select only those from qualtrics who did for bonus

type_index <- which(names(dat_summary) == 'type')

temp <- dat_summary[, c(type_index, 18:45)]  

# ggplot(temp) +
#   facet_grid ()
# geom_density(aes(Result, group = type, col = type)) + 
#   xlab(title_name)
# 
# p <- ggplot(temp)
# for (i in 18:45)
# {
#   temp <- dat_summary[, c(type_index, i)]  
#   title_name <- names(temp)[2]
#   names(temp)[2] <- 'Result'
#   
#   ggplot(temp) +
#     geom_density(aes(Result, group = type, col = type)) + 
#     xlab(title_name)
# }
# dev.off()



tp <- matrix(NA, ncol = 20, nrow = 101)
fp <- matrix(NA,ncol = 20, nrow = 101)
fn <- matrix(NA,ncol = 20, nrow = 101)
tn <- matrix(NA,ncol = 20, nrow = 101)

temp <- names(dat_summary)[c(18:31, 34, 37, 39, 41, 43, 45)]

j <- 1

for (variables in c(18:31, 34, 37, 39, 41, 43, 45))
{
  i <- 1 
  
  for (threshold in seq(0, 1, .01))
  {
    tp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    fp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'ml', na.rm = TRUE)
    fn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    tn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'ml', na.rm = TRUE)
    
    i <- i + 1
  }
  
  j <- j + 1
}

# loop through matrix for proper comps

tpr <- matrix(NA, ncol = 20, nrow = 101)
fpr <- matrix(NA,ncol = 20, nrow = 101)
fnr <- matrix(NA,ncol = 20, nrow = 101)
tnr <- matrix(NA,ncol = 20, nrow = 101)

ppv <- matrix(NA, ncol = 20, nrow = 101)
fdr <- matrix(NA,ncol = 20, nrow = 101)
npv <- matrix(NA,ncol = 20, nrow = 101)
false_or <- matrix(NA,ncol = 20, nrow = 101)

for (i in 1:20)
{
  tpr[, i] <- tp[, i] / (tp[, i] + fn[, i])
  fnr[, i] <- fn[, i] / (tp[, i] + fn[, i])
  fpr[, i] <- fp[, i] / (fp[, i] + tn[, i])
  tnr[, i] <- tn[, i] / (fp[, i] + tn[, i])
  
  ppv[, i] <- tp[, i] / (fp[, i] + tp[, i])
  fdr[, i] <- fp[, i] / (fp[, i] + tp[, i])
  npv[, i] <- tn[, i] / (tn[, i] + fn[, i])
  false_or[, i] <- fn[, i] / (tn[, i] + fn[, i])
}
fpr <- as.data.frame(fpr)
tpr <- as.data.frame(tpr)
fnr <- as.data.frame(fnr)
tnr <- as.data.frame(tnr)

ppv <- as.data.frame(ppv)
fdr <- as.data.frame(fdr)
npv <- as.data.frame(npv)
false_or <- as.data.frame(false_or)

names(fpr) <- temp
names(tpr) <- temp
names(fnr) <- temp
names(tnr) <- temp

names(ppv) <- temp
names(fdr) <- temp
names(npv) <- temp
names(false_or) <- temp

for(i in 1:20)
{
  plot(fpr[, i], tpr[, i],
       type = 's', xlim = c(0,1), ylim = c(0,1), main = names(fpr)[i],
       xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2)
  abline(a = 0, b = 1)
}

tp[6,which(temp == 'fish_combine_3_hetero_p')]
fp[6,which(temp == 'fish_combine_3_hetero_p')]
fn[6,which(temp == 'fish_combine_3_hetero_p')]
tn[6,which(temp == 'fish_combine_3_hetero_p')]

plot(fpr$fish_combine_3_hetero_p, tpr$fish_combine_3_hetero_p,
     type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Fisher combined',
     xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2)
abline(a = 0, b = 1)
roc(response = dat_summary$type, predictor = dat_summary$fish_combine_3_hetero_p)

height = (tpr$fish_combine_3_hetero_p[-1]+tpr$fish_combine_3_hetero_p[-length(tpr$fish_combine_3_hetero_p)])/2
width = diff(fpr$fish_combine_3_hetero_p) # = diff(rev(omspec))
sum(height*width)


plot(fpr$variance_sd_p_overall_homo, tpr$variance_sd_p_overall_homo,
     type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance homogeneous',
     xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2)
abline(a = 0, b = 1)
roc(response = dat_summary$type, predictor = dat_summary$variance_sd_p_overall_homo)

height = (tpr$variance_sd_p_overall_homo[-1]+tpr$variance_sd_p_overall_homo[-length(tpr$variance_sd_p_overall_homo)])/2
width = diff(fpr$variance_sd_p_overall_homo) # = diff(rev(omspec))
sum(height*width)

plot(fpr$variance_sd_p_overall_hetero, tpr$variance_sd_p_overall_hetero,
     type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance heterogeneous',
     xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2)
abline(a = 0, b = 1)
roc(response = dat_summary$type, predictor = dat_summary$variance_sd_p_overall_hetero)

height = (tpr$variance_sd_p_overall_hetero[-1]+tpr$variance_sd_p_overall_hetero[-length(tpr$variance_sd_p_overall_hetero)])/2
width = diff(fpr$variance_sd_p_overall_hetero) # = diff(rev(omspec))
sum(height*width)

plot(fpr$variance_sd_p_overall_homo, tpr$variance_sd_p_overall_homo,
     type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance homogeneous',
     xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2)
abline(a = 0, b = 1)
lines(fpr$variance_sd_p_overall_hetero, tpr$variance_sd_p_overall_hetero,
     type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance heterogeneous',
     xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2, lty = 2)

legend(x = 0, y = 1, bty = 'n', lty = c(1, 2), lwd = 2, legend = c('Homogeneous', 'Heterogeneous'))
```

<!---

# Study 2

To test the validity of statistical methods to detect data fabrication in raw data, we investigated raw data of a Stroop experiment (Stroop 1935). 

## Methods

### Data collection. 

Twenty genuine datasets were collected from the Many Labs 3 project (Ebersole et al. ).

Twenty faked datasets were collected experimentally. We sampled 20 of 84 Dutch or Flemish psychology researchers who published a peer-reviewed paper on the Stroop task between 2005-2015, which was indexed in Thomson Reuters’ Web of Science database. Dutch and Flemish researchers were selected to allow for potential follow-up on the way data was fabricated in a face-to-face interview. The period 2005-2015 was chosen to prevent a drastic decrease in the probability that the corresponding author would still be addressable via the given email. Researchers who published on the Stroop task were selected to ensure their familiarity with the type of task for which data had to be fabricated. The database was searched on October 13, 2015 and 84 unique e-mails were extracted from 86 articles.

## Results

# Discussion

- Open data for further testing (see osf.io/xxxx)
- Ecological validity
--->

# References
