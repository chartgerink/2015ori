\documentclass{article}

\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{adjustbox}
\bibliographystyle{apalike}

\title{Ecological performance of detecting data fabrication with summary statistics}
\author{Chris HJ Hartgerink, Jelte M Wicherts, Marcel ALM van Assen}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

<<prepwork, echo=FALSE,results=hide>>=
suppressPackageStartupMessages(if(!require(pROC)){install.packages('pROC')})
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(if(!require(foreign)){install.packages('foreign')})
suppressPackageStartupMessages(library(foreign))
suppressPackageStartupMessages(if(!require(latex2exp)){install.packages('latex2exp')})
suppressPackageStartupMessages(library(latex2exp))
suppressPackageStartupMessages(if(!require(ggplot2)){install.packages('ggplot2')})
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(if(!require(gridExtra)){install.packages('gridExtra')})
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(if(!require(xtable)){install.packages('xtable')})
suppressPackageStartupMessages(library(xtable))
suppressPackageStartupMessages(if(!require(httr)){install.packages('httr')})
suppressPackageStartupMessages(library(httr))

# Add ../ before running rmarkdown, or vice versa when not running there
x12 <- '../'
# x12 <- ''
@

\section*{Study 1}

<<data prepper, echo=FALSE,results=hide>>=
# Dynamically generate path names
ml_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
qualtrics_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
dat_summary_file <- sprintf('%sdata/study_01/study1_res.csv', x12)
file <- sprintf('%sdata/study_01/raw_summary_results_fabrication_qualtrics.csv', x12)
res_file <- sprintf('%sdata/study_01/qualtrics_processed.csv', x12)
ml_dat_file <- sprintf('%sdata/study_01/anchoring_ml/chjh ml1_anchoring cleaned.sav', x12)
summary_stat_file <- sprintf('%sdata/study_01/ml_summary_stats.csv', x12)
pdf_file <- sprintf('%sarchive/gender_interaction.pdf', x12)

# Set the number of iterations to use in calculations
iter <- 100000

# Compute the summary statistics for Many Labs
source(sprintf('%sfunctions/compute_summary_anch_ml.R', x12))
# Process the collected, fabricated data
source(sprintf('%sfunctions/process_qualtrics_anch_01.R', x12))
# Concatenate, analyze, and write out all ML and qualtrics data
# Uncomment this line to rerun, but take care: might take a while
if(!file.exists(sprintf('%sdata/study_01/study1_res.csv', x12)))
{
  set.seed(123);source(sprintf('%sfunctions/concatenate_analyze_01.R', x12))  
}
@

We tested the performance of statistical methods to detect data fabrication based on summary results with genuine and fabricated summary results of four anchoring studies \citep{tversky1974,jacowitz1995}. The anchoring effect is a well-known psychological heuristic that uses the information in the question as the starting point for the answer, which is then adjusted to yield a final estimate of a quantity. For example 'Is the percentage of African countries in the United Nations more or less than [10\% or 65\%]?'. These questions yield mean responses of 25\% and 45\%, respectively \citep{tversky1974}, despite essentially posing the same factual question. A considerable amount of genuine datasets on this heuristic are freely available and we collected fabricated datasets within this study.

\subsection*{Methods}

The four anchoring studies for which results were collected were (i) distance from San Francisco to New York, (ii) population of Chicago, (iii) height of the Mount Everest, and (iv) the number of babies born per day in the United States. Each of the four studies provided summary results for a 2 (low/high anchoring) $\times$ 2 (male/female) factorial design. Throughout this study, the unit of analysis is a set of summary statistics (i.e., means, standard deviations, and test results) for the four anchoring studies from one respondent. For current purposes, a respondent is defined as researcher/lab where the four anchoring studies' summary statistics originate from. All materials, data, and analyses scripts are freely available on the OSF (\url{osf.io/b24pq}) and were preregistered (\url{osf.io/ejf5x}; deviations are explicated in this report).

\subsubsection*{Data collection}

We downloaded thirty-six genuine datasets from the publicly available Many Labs (ML) project \citep[\url{osf.io/pqf9r};][]{klein2014}. The ML project replicated several effects across thirty-six locations, including the anchoring effect in the four studies mentioned previously. Considering the size of the ML project, the transparency of research results, and minimal individual gain for fraud, we assumed these data to be genuine. For each of the thirty-six locations, sample sizes, means, and standard deviations (four each) were computed for each of the four conditions in the four anchoring studies across the thirty-six locations (i.e., $3\times4\times4\times36$). We computed these summary statistics from the raw ML data, which were cleaned using the original analysis scripts from the ML project.

Using quotum sampling, we collected thirty-six fabricated datasets of summary results for all four anchoring studies. Quotum sampling was applied to sample as many responses as possible for the available 36 rewards (i.e., not all respondents might request the gift card and count towards the quotum; one participant did not request a reward). The sampling frame consisted of 2,038 psychology researchers who published a peer-reviewed paper in 2015, as indexed in the Web of Science (WoS) with the filter set to the U.S. We sampled psychology researchers to improve familiarity with the anchoring effect \citep{jacowitz1995,tversky1974}, for which summary results were fabricated. We filtered for U.S. researchers to ensure familiarity with the imperial measurement system, which is the scale of some of the anchoring studies (note: we found out several non-U.S. researchers were included because this filter also retained papers with co-authors from the U.S.). WoS was searched on October 13, 2015. In total, 2,038 unique corresponding e-mails were extracted from 2,014 papers (due to multiple corresponding authors).

A random sample of 1,000 researchers were approached via e-mail to participate in this study on April 25, 2016 (invitation: \url{osf.io/s4w8r}). The study took place via Qualtrics with anonimization procedures in place (e.g., no IP-addresses saved). We informed the participating researchers that the study would require them to fabricate data and explicitly mentioned that we would investigate these data with statistical methods to detect data fabrication. We also clarified to the  respondents that they could stop at any time without providing a reason. If they wanted, respondents received a \$30 Amazon gift card as compensation for their participation if they were willing to enter their email address. They could win an additional \$50 Amazon gift card if they were one of three top fabricators. The provided email addresses were unlinked from individual responses upon sending the bonus gift cards. The full text of the Qualtrics survey is available at \url{osf.io/w984b}.

Each respondent was instructed to fabricate 32 summary statistics (4 studies $\times$ 2 conditions $\times$ 2 sexes $\times$ 2 statistics [mean and sd]) that fulfilled three hypotheses. We instructed respondents to fabricate results for the hypotheses (i) main effect of condition, (ii) no effect of sex, and (iii) no interaction effect between condition and sex. Respondents did not need to fabricate sample sizes, which were set to 25 per cell a priori. The fabricated summary statistics and their accompanying test results for these three hypotheses serve as the data to examine the properties of tools to detect data fabrication.

We provided respondents with a template spreadsheet to fill out the fabricated data, in order to standardize the fabrication process without restraining the participant in how they chose to fabricate data. Figure \ref{fig1} depicts an example of this spreadsheet (original:  \url{osf.io/w6v4u}). We requested respondents to fill in the yellow cells with fabricated data, which includes means and the standard deviations for four conditions. Using these values, statistical tests are computed and shown in the "Current result" column instantaneously. If these results confirmed the hypotheses, a checkmark appeared as depicted in Figure \ref{fig1}. We required respondents to copy-paste the yellow cells into Qualtrics, to provide a standardized response format that could be automatically processed in the analyses.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/spreadsheet.png}
\caption{Example of a filled in template spreadsheet used in the fabrication process. Respondents fabricated data in the yellow cells, which were used to compute the results of the hypothesis tests. If the fabricated data confirm the hypotheses, a checkmark appeared in a green cell.}
\label{fig1}
\end{center}
\end{figure}

Upon completing the fabrication of the data, respondents were debriefed. Respondents answered several questions about their statistical knowledge and approach to data fabrication and finally we reminded them that data fabrication is widely condemned by professional organizations, institutions, and funding agencies alike. We rewarded participation with a \$30 Amazon gift card and the fabricated results that were most difficult to detect received a bonus \$50 Amazon gift card.

\subsubsection*{Data analysis}

To detect data fabrication in a set of summary results, we first tested the standardized standard deviations (SDs) for data fabrication \citep{simonsohn2013} across the four anchoring studies. This method tests whether the observed SDs contain a reasonable amount of variation, as expected based on random sampling processes. For example, if four independent samples all yield the variance $2.22$, this could be considered excessively consistent when the probability that this amount of consistency (or more) is less than 1 out of 1000 in truly random samples. To compute this probability, we first standardized the SDs for each of the four studies with
\begin{equation}
z_j=\sqrt{\frac{s^2_j}{MS_w}}=\sqrt{\frac{s^2_j}{\left(\frac{\sum\limits^k_{j=1}(N_j-1)s^2_j}{\sum\limits^k_{j=1}(N_j-1)}\right)}}
\label{s2_j}
\end{equation}
where $z_j$ denotes the standardized SD in group $j$ ($MS_w$ is the simple arithmetic mean when sample sizes are equal for all cells, which is the case for the fabricated datasets). We tested different measures to detect data fabrication that utilize these standardized SDs (i.e., $z_j$). We included the variance of the standardized SDs \citep[i.e.,  $SD_{z}$;][]{simonsohn2013} and tried out the max-min distance of the standardized SDs (denoted $max-min_{z}$) as an alternative measure. We compared the observed value for each measure with the expected distribution when the summary results are used to generate random samples. To this end, we simulated the expected distribution of standardized SDs and computed the expected distribution of each measure. This expected distribution was used to determine the $p$-value of the observed $SD_z$ and $max-min_z$. We simulated the standardized variance for each of the $j$ groups as
\begin{equation}
z^2_j\sim\left(\frac{\chi^2_{N_j-1}}{N_j-1}\right)/MS_w
\label{simvar}
\end{equation}
These simulated values are used to compute the expected distribution of the $SD_z$ and $max-min_z$  measures.

Testing the standardized SDs for potential data fabrication can be done either for each study separately or all studies combined; the test can also be done under different assumptions of population variances across conditions. The assumptions of population variance can either by that all SDs originate from the same distribution \citep[as in][]{simonsohn2013}, the SDs within a factor are from the same distribution, or each group comes from its own distribution. We preregistered the method that assumes the SDs are drawn from the same distribution for the various conditions (i.e., homogeneous SDs) and are tested across all studies. However, upon conducting the analyses, we decided homogenous SDs are not unequivocal and included computations where the SDs for the low anchor and high anchor are from different distributions (i.e., heterogeneous SDs). Additionally, the signal for data fabrication across the four anchoring studies might result in different studies cancelling each other out, so we also included analyses where each study was analyzed separately.

Second, we applied the reversed Fisher method to detect data fabrication to the nonsignificant $p$-values twice: once for the results of gender effects hypothesis in each study and once for the results of the interaction effect hypothesis for each study. The Fisher method \citep{fisher1925} tests for evidence of an effect in a set of $p$-values by testing for a right-skew $p$-value distribution, but we adjusted it here to test for results that are overly consistent with the null hypothesis and result in a left-skew distribution (see Figure \ref{leftskew}). The original Fisher method is computed as
\begin{equation}
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(p_i)
\label{fishertest}
\end{equation}
and tests for right-skew in a set of $p$-values, but we adjust it to the following
\begin{equation}
\chi^2_{2k}=-2\sum\limits^k_{i=1}\ln(1-\frac{p_i-t}{1-t})
\label{fisheradjusted}
\end{equation}
where it now tests for left-skew (i.e., more larger $p$-values than smaller $p$-values) across the $k$ number of $p$-values that falls above the threshold $t$. We set this threshold to .05 in order to include only nonsignificant test results. The theoretical idea behind this method is that researchers who fabricate nonsignificant data might forget to fabricate a uniform $p$-value distribution, given the frequent misinterpretation of $p$-values \citep[e.g., as the probability of an effect,][]{Goodman2008135,Altman_1995}. % I can add more refs but it is just a rabbit hole to start with more.

<<figure, echo = FALSE, results = hide>>=
if(!file.exists(sprintf('%sfigures/fisherfig.pdf', x12)))
{
  pdf(sprintf('%sfigures/fisherfig.pdf', x12), width = 14, height = 7)
  par(mar = c(5, 5, 3, 1), mfrow = c(1, 2))
  
  plot(dbeta(seq(0, 1, .001), .05, 1),
       x = seq(0, 1, .001), type = 'l',
       bty = 'n', xlab = 'P-value', ylab = 'Density',
       main = '(A) Right-skew (normal)')
  lines(dunif(seq(0, 1, .001)),
        x = seq(0, 1, .001), lty = 2)
  plot(dbeta(seq(0, 1, .001), 1, .05),
       x = seq(0, 1, .001), type = 'l',
       bty = 'n', xlab = 'P-value', ylab = 'Density',
       main = '(B) Left-skew (potentially anomalous)')
  lines(dunif(seq(0, 1, .001)),
        x = seq(0, 1, .001), lty = 2)
  abline(v = .05, lty = 3)
  text(x = .15, y = 33, labels = "<- Threshold")
  dev.off()
}
@

% add plot of left and right skew
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/fisherfig.pdf}
\caption{Conceptual representation of what the Fisher test inspects (Equation \ref{fishertest}; panel A) and what the adjusted Fisher test inspects (Equation \ref{fisheradjusted}; panel B). Both panels test whether there is sufficient evidence that the solid line deviates from the dashed line, except that the type of deviation that the test is sensitive to is the exact opposite.}
\label{leftskew}
\end{center}
\end{figure}

Finally, we combined the aforementioned methods to detect data fabrication with the Fisher method. This included the $SD_z$ measure across all studies and the Fisher test (Equation \ref{fisheradjusted}) of the gender hypothesis test and the interaction test.  We expected this combination test of the three individual tests for data fabrication to be more powerful than the individual tests, given that these tests inspect different manifestations of data fabrications. Based on the results of the combined test results, the three least detectable data fabricators were selected. The three respondents with the highest $p$-values on this combined method to detect data fabrication contained the least evidential value for deviating from genuine data and received an additional \$50 Amazon gift card.

For each of these four tests to detect data fabrication ($SD_z$, Fisher test for the gender and interaction hypotheses, combined methods) we carried out sensitivity and specificity analyses using ROC-curves. ROC-curves indicate the sensitivity (i.e., True Positive Rate [TPR]) and specificity (i.e., True Negative Rate [TNR]) for various decision criteria (e.g., $\alpha=0, .01, .02, ..., .99, 1$). With these ROC-curves, informed decisions about optimal alpha levels can be made based on various criteria. In this case, we determine the optimal alpha level by finding that alpha level for which the combination of TPR and TNR were highest. For example, if $\alpha=.04$ results in $TPR=.30$ and $TNR=.70$, but $\alpha=.05$ results in $TPR=.5$ and $TNR=.5$, .05 was chosen as an optimal decision criterion based on the sample. 

\subsection*{Results}

<<read in data, echo = FALSE, results = hide>>=
# Read in datafile, just in case the computations were not re-run.
dat_summary <- read.csv(dat_summary_file)
# Select only those from qualtrics who did for bonus
# removed <- sum(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus))
# dat_summary <- dat_summary[!(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus)), ]

type_index <- which(names(dat_summary) == 'type')
temp <- dat_summary[, c(type_index, 18:length(names(dat_summary)))]  
dat_summary$es_gender <- sqrt(dat_summary$es2_gender)
dat_summary$es_condition <- sqrt(dat_summary$es2_condition)
dat_summary$es_interaction <- sqrt(dat_summary$es2_interaction)

# naming just for ease of use later
dat_summary$es_p <- 1 - dat_summary$es_condition

if(!file.exists(sprintf('%sfigures/ddfab_density.pdf', x12)))
{
  pdf(sprintf('%sfigures/ddfab_density.pdf', x12), width = 10, height = 7)
  # Gender
  plot_p_gender <- ggplot(dat_summary, aes(x = p_gender, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("$P$-value")) + 
    ylab("Density") + 
    ylim(0, 1.5) + 
    scale_fill_discrete(guide=FALSE) + 
    ggtitle('Gender')
  
  plot_es_gender <- ggplot(dat_summary, aes(x = es_gender, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  # Condition
  plot_p_condition <- ggplot(dat_summary, aes(x = p_condition, fill = type)) +
    geom_density(alpha = .3) +
    xlim(0, 1) +
    xlab(TeX("$P$-value")) +
    ylab("Density") +
    scale_fill_discrete(labels = c("Genuine","Fabricated")) + 
    theme(legend.position="top") + 
    ggtitle('Condition')
  
  plot_p_condition$labels$fill <- ""
  
  plot_es_condition <- ggplot(dat_summary, aes(x = es_condition, fill = type)) + 
    geom_density(alpha = .3) + 
    xlim(0, 1) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  
  # Interaction
  plot_p_interaction <- ggplot(dat_summary, aes(x = p_interaction, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("$P$-value")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE) +
    ylim(0, 1.5) +
    ggtitle('Interaction')
  
  plot_es_interaction <- ggplot(dat_summary, aes(x = es_interaction, fill = type)) + 
    geom_density(alpha = .3) + 
    xlab(TeX("Effect size ($r$)")) + 
    ylab("Density") + 
    scale_fill_discrete(guide=FALSE)
  
  # Plot
  grid.arrange(plot_p_gender,
               plot_p_condition,
               plot_p_interaction,
               plot_es_gender,
               plot_es_condition,
               plot_es_interaction,
               ncol = 3)
  dev.off()
}
@

The collected data included \Sexpr{table(dat_summary$type)[1]} genuine data from Many Labs 1 \citep[\url{osf.io/pqf9r};][]{klein2014} and \Sexpr{table(dat_summary$type)[2]} fabricated datasets(\url{osf.io/e6zys}; \Sexpr{sum(dat_summary$type == 'qualtrics' & is.na(dat_summary$bonus))} participants did not participate for a bonus). 

Figure \ref{densities} shows a group-level comparison of the genuine- and fabricated $p$-values and effect sizes ($r$). Such group-level comparisons provide an overview of the differences between the genuine- and fabricated data \citep[see also][]{Akhtar-Danesh2003}. These distributions indicate little group differences between genuine- and fabricated data when nonsignificant effects are inspected (i.e., gender and interaction hypotheses) whereas there seem to be large group differences when we required subjects to fabricate significant data (i.e., condition hypothesis). From our own experience, and anecdotal evidence elsewhere \citep{BAILEY1991741}, large effects have previously raised initial suspicions. These data corroborate the idea that extremely large effect sizes (e.g., $r>.95$) might prove to be an easy-to-implement flag for potentially anomalous data (it is wise to seek for alternative explanations after flagging, however). Considering this, we also investigated how well effect sizes perform in detecting data fabrication (not preregistered). In the following sections, we investigate the performance of such statistical methods to detect data fabrication on an respondent-level basis.

% add plot of left and right skew
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/ddfab_density.pdf}
\caption{Overlay of density distributions for both genuine and fabricated data, per effect and type of result. We instructed respondents to fabricate nonsignificant data for the gender and interaction effects, and a significant effect for the condition effect.}
\label{densities}
\end{center}
\end{figure}

<<generate results p/method, echo = FALSE, results = hide>>=
tp <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fp <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fn <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
tn <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

temp <- names(dat_summary)[grepl(names(dat_summary), pattern = "_p")]

j <- 1

for (variables in which(grepl(names(dat_summary), pattern = "_p")))
{
  i <- 1 
  
  for (threshold in seq(0, 1, .01))
  {
    tp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    fp[i,j] <- sum(dat_summary[, variables] <= threshold & dat_summary$type == 'ml', na.rm = TRUE)
    fn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'qualtrics', na.rm = TRUE)
    tn[i,j] <- sum(dat_summary[, variables] > threshold & dat_summary$type == 'ml', na.rm = TRUE)
    
    i <- i + 1
  }
  
  j <- j + 1
}

# loop through matrix for proper comps

tpr <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fpr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fnr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
tnr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

ppv <- matrix(NA, ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
fdr <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
npv <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)
false_or <- matrix(NA,ncol = sum(grepl(names(dat_summary), pattern = "_p")), nrow = 101)

for (i in 1:sum(grepl(names(dat_summary), pattern = "_p")))
{
  tpr[, i] <- tp[, i] / (tp[, i] + fn[, i])
  fnr[, i] <- fn[, i] / (tp[, i] + fn[, i])
  fpr[, i] <- fp[, i] / (fp[, i] + tn[, i])
  tnr[, i] <- tn[, i] / (fp[, i] + tn[, i])
  
  ppv[, i] <- tp[, i] / (fp[, i] + tp[, i])
  fdr[, i] <- fp[, i] / (fp[, i] + tp[, i])
  npv[, i] <- tn[, i] / (tn[, i] + fn[, i])
  false_or[, i] <- fn[, i] / (tn[, i] + fn[, i])
}
fpr <- as.data.frame(fpr)
tpr <- as.data.frame(tpr)
fnr <- as.data.frame(fnr)
tnr <- as.data.frame(tnr)

ppv <- as.data.frame(ppv)
fdr <- as.data.frame(fdr)
npv <- as.data.frame(npv)
false_or <- as.data.frame(false_or)

names(fpr) <- temp
names(tpr) <- temp
names(fnr) <- temp
names(tnr) <- temp

names(ppv) <- temp
names(fdr) <- temp
names(npv) <- temp
names(false_or) <- temp


height = (tpr[-1, ] + tpr[-dim(tpr)[1], ])/2
width = apply(fpr, 2, diff) # = diff(rev(omspec))
auroc_res <- apply(height * width, 2, sum)

Method <- c("$SD_z$ homogeneous all",
                    "$SD_z$ study 1",
                    "$SD_z$ study 2",
                    "$SD_z$ study 3",
                    "$SD_z$ study 4",
                    "$maxmin_z$ homogeneous all",
                    "$maxmin_z$ study 1",
                    "$maxmin_z$ study 2",
                    "$maxmin_z$ study 3",
                    "$maxmin_z$ study 4",
                    "$SD_z$ heterogeneous all",
                    "$SD_z$ study 1 low condition",
                    "$SD_z$ study 1 high condition",
                    "$SD_z$ study 2 low condition",
                    "$SD_z$ study 2 high condition",
                    "$SD_z$ study 3 low condition",
                    "$SD_z$ study 3 high condition",
                    "$SD_z$ study 4 low condition",
                    "$SD_z$ study 4 high condition",
                    "$maxmin_z$ heterogeneous all",
                    "$maxmin_z$ study 1 low condition",
                    "$maxmin_z$ study 1 high condition",
                    "$maxmin_z$ study 2 low condition",
                    "$maxmin_z$ study 2 high condition",
                    "$maxmin_z$ study 3 low condition",
                    "$maxmin_z$ study 3 high condition",
                    "$maxmin_z$ study 4 low condition",
                    "$maxmin_z$ study 4 high condition",
                    "Fisher test gender hypothesis",
                    "Fisher test interaction hypothesis",
                    "Combined Fisher test (3 results, $SD_z$ homogeneous)",
                    "Combined Fisher test (3 results, $SD_z$ heterogeneous)",
                    "Combined Fisher test (6 results, $SD_z$ homogeneous)",
                    "Combined Fisher test (6 results, $SD_z$ heterogeneous)",
                    "Effect sizes (1-r)")
auroc <- as.numeric(auroc_res)
# print(xtable(data.frame(Method, auroc),
#              caption = "Area Under the Receiver Operating Curve (AUROC) for the various statistical methods used to detect data fabrication. Homogeneous: assumes one population variance underlying all groups. Heterogeneous: assumes separate population variance per anchoring condition.",
#              label = "auroc", digits = 3), include.rownames = FALSE, sanitize.text.function=identity)

@

\subsubsection*{Performance of using SDs to detect data fabrication}

We applied two different operationalizations to inspect for data fabrication in the genuine- and fabricated datasets based on variance in the SDs. The $SD_z$ method \citep{simonsohn2013} inspects whether there the variance of the SDs themselves varies sufficiently, and we tested another method that inspects the range of the SDs. Table \ref{auroc_variances} indicates that both methods have similar performance when inspected with the Area Under the Receiver Operating Curve (AUROC). 

The AUROC value indicates the probability that a randomly drawn fabricated dataset is classified as fabricated before a randomly drawn genuine dataset is classified as fabricated \citep{doi:10.1148/radiology.143.1.7063747}. In other words, if $AUROC=.5$ this indicates that correctly classifying a randomly drawn dataset in this sample is equal to a coin flip. Considering this, we will regard any AUROC$<.6$ as plainly insufficient for detecting data fabrication, $.6\leq AUROC<.7$ as failed, $.7\leq AUROC<.8$ as sufficient, $.8\leq AUROC<.9$ as good, and $.9\leq AUROC\leq 1$ as excellent.

<<auroc for variance methods, echo=FALSE, results=tex>>=
selmaxmin <- grepl(Method, pattern = '^.maxmin_')
selsd <- grepl(x = Method, pattern = '^.SD_z')
row_table <- c("Homogeneous, combined",
                    "Homogeneous, study 1",
                    "Homogeneous, study 2",
                    "Homogeneous, study 3",
                    "Homogeneous, study 4",
                    "Heterogeneous, combined",
                    "Heterogeneous study 1, low anchor condition",
                    "Heterogeneous study 1, high anchor condition",
                    "Heterogeneous study 2, low anchor condition",
                    "Heterogeneous study 2, high anchor condition",
                    "Heterogeneous study 3, low anchor condition",
                    "Heterogeneous study 3, high anchor condition",
                    "Heterogeneous study 4, low anchor condition",
                    "Heterogeneous study 4, high anchor condition")
variances_table <- data.frame(Method = row_table, SDz = auroc[selsd], maxminz = auroc[selmaxmin])
names(variances_table) <- c("Method", "auroc $SD_z$", "auroc $maxmin_z$")
print(xtable(variances_table,
             caption = "Area Under the Receiver Operating Curve (AUROC) for the two statistical methods used to detect data fabrication based on variances. Homogeneous: assumes one population variance underlying all groups. Heterogeneous: assumes separate population variance per anchoring condition.",
             label = "auroc_variances", digits = 3), include.rownames = FALSE, sanitize.text.function=identity)
@


<<generate variance ROC, echo=FALSE, results=hide>>=
if(!file.exists(sprintf('%sfigures/varianceplot.pdf', x12)))
{
  pdf(sprintf('%sfigures/varianceplot.pdf', x12), width = 8, height = 6)
  plot(fpr$variance_sd_p_overall_homo, tpr$variance_sd_p_overall_homo,
       type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance analysis',
       xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2, lty = 2)
  abline(a = 0, b = 1)
  lines(fpr$variance_sd_p_overall_hetero, tpr$variance_sd_p_overall_hetero,
        type = 's', xlim = c(0,1), ylim = c(0,1), main = 'Variance heterogeneous',
        xlab = 'Sensitivity / TPR', ylab = 'Specificity / TNR', lwd = 2, lty = 1)
  legend(x = 0, y = 1, bty = 'n', lty = c(2, 1), lwd = 2, legend = c('Homogeneous', 'Heterogeneous'))
  dev.off()
}
@

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../figures/varianceplot.pdf}
\caption{PLACEHOLDER}
\label{varianceplot}
\end{center}
\end{figure}

\subsubsection*{Performance of using $p$-values to detect data fabrication}

We asked researchers to fabricate data for nonsignificant effect sizes after inspecting whether the genuine data resembled a uniform distribution. Consequently,  However, as Table \ref{auroc_fisher} indicates, the auroc for these methods are hardly better than chance. In other words, for effects that are specifically fabricated to be nonsignificant, researchers might not fabricate such large $p$-values to be suspicious.

<<auroc for single fisher methods, echo=FALSE, results=tex>>=
sel <- grepl(Method, pattern = '^(Fisher test)')
row_table <- Method[sel]
tabl <- data.frame(Method = row_table, auroc = auroc[sel])
print(xtable(tabl,
             caption = "Area Under the Receiver Operating Curve (AUROC) for PLACEHOLDER",
             label = "auroc_fisher", digits = 3), include.rownames = FALSE, sanitize.text.function=identity)
@

\subsubsection*{Performance of combining SDs and $p$-value methods to detect data fabrication}


<<auroc for combined fisher methods, echo=FALSE, results=tex>>=
sel <- grepl(Method, pattern = 'Combined Fisher test')
row_table <- Method[sel]
tabl <- data.frame(Method = row_table, auroc = auroc[sel])
print(xtable(tabl,
             caption = "Area Under the Receiver Operating Curve (AUROC) for PLACEHOLDER",
             label = "auroc_combined", digits = 3), include.rownames = FALSE, sanitize.text.function=identity)
@


\subsubsection*{Performance of large effect sizes to detect data fabrication}

<<get massive statcheck data for checks, echo=FALSE, results=hide>>=
if(!file.exists(sprintf('%sdata/statcheck_dataset.csv', x12)))
{
  url <- 'https://github.com/chartgerink/2016statcheck_data/raw/master/statcheck_dataset.csv'
  # If link is broken, more persistent version availabl at
  # http://dx.doi.org/10.17026/dans-2cm-v9j9

  GET(url,
      write_disk(sprintf('%sdata/statcheck_dataset.csv', x12),
                 overwrite = TRUE))
}

statcheck_data <- read.csv(sprintf('%sdata/statcheck_dataset.csv', x12))
# Compute the effect sizes
sel <- statcheck_data$Statistic == 'F'
statcheck_data$es[sel] <- sqrt((statcheck_data$Value[sel] *
                                  (statcheck_data$df1[sel] /
                                     statcheck_data$df2[sel])) /
                                 (((statcheck_data$Value[sel] *
                                      statcheck_data$df1[sel]) /
                                     statcheck_data$df2[sel]) + 1))

sel <- statcheck_data$Statistic == 't'
statcheck_data$es[sel] <- sqrt((statcheck_data$Value[sel]^2 *
                                  (1 / statcheck_data$df2[sel])) /
                                 (((statcheck_data$Value[sel]^2 * 1) /
                                     statcheck_data$df2[sel]) + 1))

sel <- statcheck_data$Statistic == 'r' &
  statcheck_data$Value <= 1 &
  statcheck_data$Value > 0
statcheck_data$es[sel] <- statcheck_data$Value[sel]
@

%The \Sexpr{sum(dat_summary$es_condition > max(dat_summary$es_condition[dat_summary$type == 'ml']))} largest significant effects were fabricated, with \Sexpr{sum(dat_summary$es_condition[dat_summary$type == 'qualtrics'] > .9)} (\Sexpr{round(sum(dat_summary$es_condition[dat_summary$type == 'qualtrics'] > .9) / sum(dat_summary$type == 'qualtrics'), 3)}\%) larger than $r=.9$. Based on descriptive data throughout psychology \citep{Hartgerink2016-lg}, effect sizes $\geq.9$ belong to the \Sexpr{round(sum(statcheck_data$es[!is.na(statcheck_data$es)] < .9) / length(statcheck_data$es[!is.na(statcheck_data$es)])*100, 0)}th percentile of effects throughout the psychology literature and effect sizes $\geq.95$ \Sexpr{round(sum(statcheck_data$es[!is.na(statcheck_data$es)] < .95) / length(statcheck_data$es[!is.na(statcheck_data$es)])*100, 0)}th percentile.


\subsection*{Discussion}

% variance analysis and effect size best ways to inspect
% but severely correlated: smaller variances = larger effects
% easiest way is too look for massive effects in the effects

% Simonsohn method not invalid under homogeneous, just somethign to take into account
% easily adjusted

% Discuss how the PPV is likely to be overestimated because prevalence is 50% in this study
% How we don't know the prevalence, in fact.

% Discuss alpha choice based on sample has large error (small sample).
% Only provides a first indication

% Comparison between methods not tested, large error given small sample.
% This provides just initial work


\bibliography{../bibliography/library}
\newpage
%------------------------------------
%handy to include this at the end
%------------------------------------
\section*{SessionInfo}
%-------------------------------------

<<sessionInfo>>=

sessionInfo()

@ 

\end{document}