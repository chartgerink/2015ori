
@ARTICLE{fanelli2009,
  title       = {How many scientists fabricate and falsify research? A
                 systematic review and meta-analysis of survey data},
  author      = "Fanelli, D",
  journal     = "PloS one",
  volume      =  4,
  number      =  5,
  pages       = "e5738",
  year        =  2009,
  doi         = "10.1371/journal.pone.0005738"
}

@ARTICLE{pupovac2015,
  title       = {Scientists admitting to plagiarism: A meta-analysis of surveys},
  author      = "Pupovac, Vanja and Fanelli, Daniele",
  journal     = "Science and engineering ethics",
  volume      =  21,
  number      =  5,
  pages       = "1331--1352",
  month       =  oct,
  year        =  2015,
  url         = "http://dx.doi.org/10.1007/s11948-014-9600-6",
  keywords    = "Data fabrication; Data falsification; Plagiarism; Research
                 integrity; Research misconduct; Survey methodology",
  issn        = "1353-3452, 1471-5546",
  pmid        = "25352123",
  doi         = "10.1007/s11948-014-9600-6"
}


@ARTICLE{jacowitz1995,
  title     = {Measures of anchoring in estimation tasks},
  author    = "Jacowitz, Karen E and Kahneman, Daniel",
  journal   = "Personality \& social psychology bulletin",
  publisher = "SAGE PERIODICALS PRESS",
  volume    =  21,
  pages     = "1161--1166",
  year      =  1995,
  url       = "http://facweb.plattsburgh.edu/wendy.braje/students/psy205/JKarticle.pdf",
  issn      = "0146-1672"
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{klein2014,
  title   = {Investigating variation in replicability},
  author  = "Klein, Richard A and Ratliff, Kate A and Vianello, Michelangelo
             and Jr., Reginald B Adams and Bahn\'{\i}k, \v{S}t\v{e}p\'{a}n and
             Bernstein, Michael J and Bocian, Konrad and Brandt, Mark J and
             Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep
             and Chandler, Jesse and Cheong, Winnee and Davis, William E and
             Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and
             Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and
             Hicks, Joshua A and Hovermale, James F and Hunt, S Jane and
             Huntsinger, Jeffrey R and IJzerman, Hans and John, Melissa-Sue and
             Joy-Gaba, Jennifer A and Kappes, Heather Barry and Krueger, Lacy E
             and Kurtz, Jaime and Levitan, Carmel A and Mallett, Robyn K and
             Morris, Wendy L and Nelson, Anthony J and Nier, Jason A and
             Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M and
             Schmidt, Kathleen and Skorinko, Jeanine L and Smith, Robert and
             Steiner, Troy G and Storbeck, Justin and Swol, Lyn M Van and
             Thompson, Donna and ‘t Veer, A E van and Vaughn, Leigh Ann and
             Vranka, Marek and Wichman, Aaron L and Woodzicka, Julie A and
             Nosek, Brian A",
  journal = "Social psychology",
  volume  =  45,
  number  =  3,
  pages   = "142--152",
  year    =  2014,
  url     = "http://dx.doi.org/10.1027/1864-9335/a000178",
  eprint  = "http://dx.doi.org/10.1027/1864-9335/a000178",
  issn    = "0147-829X",
  doi     = "10.1027/1864-9335/a000178"
}


@ARTICLE{simonsohn2013,
  title       = {Just post it: The lesson from two cases of fabricated data
                 detected by statistics alone},
  author      = "Simonsohn, Uri",
  affiliation = "The Wharton School, University of Pennsylvania.",
  journal     = "Psychological science",
  volume      =  24,
  number      =  10,
  pages       = "1875--1888",
  month       =  oct,
  year        =  2013,
  url         = "http://dx.doi.org/10.1177/0956797613480366",
  keywords    = "data posting; data sharing; decision making; fake data;
                 judgment; scientific communication",
  issn        = "0956-7976, 1467-9280",
  pmid        = "23982243",
  doi         = "10.1177/0956797613480366"
}


@BOOK{fisher1925,
  title     = {Statistical methods for research workers},
  author    = "Fisher, Ronald Aylmer",
  publisher = "Oliver Boyd",
  year      =  1925,
  address   = "Edinburg, United Kingdom"
}


@ARTICLE{hong2008,
  title       = {A comparison of meta-analysis methods for detecting
                 differentially expressed genes in microarray experiments},
  author      = "Hong, Fangxin and Breitling, Rainer",
  affiliation = "Department of Biostatistics, Division of Information Sciences,
                 City of Hope National Medical Center, Beckman Research
                 Institute, 1500 Duarte Rd, Duarte, CA 91010, USA.
                 fxhong@jimmy.harvard.edu",
  journal     = "Bioinformatics",
  volume      =  24,
  number      =  3,
  pages       = "374--382",
  month       =  "1~" # feb,
  year        =  2008,
  url         = "http://dx.doi.org/10.1093/bioinformatics/btm620",
  issn        = "1367-4803, 1367-4811",
  pmid        = "18204063",
  doi         = "10.1093/bioinformatics/btm620"
}


@ARTICLE{rocr,
  title       = {{ROCR}: visualizing classifier performance in {R}},
  author      = "Sing, Tobias and Sander, Oliver and Beerenwinkel, Niko and
                 Lengauer, Thomas",
  affiliation = "Department of Computational Biology and Applied Algorithmics,
                 Max-Planck-Institute for Informatics, Saarbr{\"{u}}cken,
                 Germany. tobias.sing@mpi-sb.mpg.de",
  journal     = "Bioinformatics",
  volume      =  21,
  number      =  20,
  pages       = "3940--3941",
  month       =  "15~" # oct,
  year        =  2005,
  url         = "http://dx.doi.org/10.1093/bioinformatics/bti623",
  issn        = "1367-4803",
  pmid        = "16096348",
  doi         = "10.1093/bioinformatics/bti623"
}

  
@ARTICLE{proc,
  title       = {{pROC}: an open-source package for {R} and S+ to analyze and
                 compare {ROC} curves},
  author      = "Robin, Xavier and Turck, Natacha and Hainard, Alexandre and
                 Tiberti, Natalia and Lisacek, Fr\'{e}d\'{e}rique and Sanchez,
                 Jean-Charles and M{\"{u}}ller, Markus",
  affiliation = "Biomedical Proteomics Research Group, Department of Structural
                 Biology and Bioinformatics, Medical University Centre, Geneva,
                 Switzerland. Xavier.Robin@unige.ch",
  journal     = "BMC bioinformatics",
  volume      =  12,
  pages       = "77",
  month       =  "17~" # mar,
  year        =  2011,
  url         = "http://dx.doi.org/10.1186/1471-2105-12-77",
  issn        = "1471-2105",
  pmid        = "21414208",
  doi         = "10.1186/1471-2105-12-77",
  pmc         = "PMC3068975"
}


@ARTICLE{tversky1974,
  title   = {Judgment under uncertainty: Heuristics and biases},
  author  = "Tversky, A and Kahneman, D",
  journal = "Science",
  volume  =  185,
  number  =  4157,
  pages   = "1124--1131",
  month   =  "27~" # sep,
  year    =  1974,
  url     = "http://dx.doi.org/10.1126/science.185.4157.1124",
  issn    = "0036-8075",
  pmid    = "17835457",
  doi     = "10.1126/science.185.4157.1124"
}


@BOOK{Schafer2004-yt,
  title     = {Automatic identification of faked and fraudulent interviews in
               surveys by two different methods},
  author    = "Sch{\"{a}}fer, Christin and Schr{\"{a}}pler, J{\"{o}}rg-Peter
               and M{\"{u}}ller, Klaus-Robert and Wagner, Gert G",
  publisher = "Deutsches Institut f{\"{u}}r Wirtschaftsforschung",
  year      =  2004
}

@ARTICLE{Finn2015-ag,
  title   = {Genuine fakes: The prevalence and implications of data fabrication
             in a large South African survey},
  author  = "Finn, Arden and Ranchhod, Vimal",
  journal = "The World Bank economic review",
  month   =  "27~" # sep,
  year    =  2015,
  url     = "http://wber.oxfordjournals.org/content/early/2015/09/27/wber.lhv054.abstract",
  issn    = "0258-6770",
  doi     = "10.1093/wber/lhv054"
}

# ddfab
# fraud
# clinical trials
@article{BAILEY1991741,
title = "Detecting fabrication of data in a multicenter collaborative animal study",
journal = "Controlled Clinical Trials",
volume = "12",
number = "6",
pages = "741 - 752",
year = "1991",
note = "Large effects were on of the suspicions",
issn = "0197-2456",
doi = "http://dx.doi.org/10.1016/0197-2456(91)90037-M",
url = "http://www.sciencedirect.com/science/article/pii/019724569190037M",
author = "Kent R. Bailey",
keywords = "Evidence",
keywords = "lack of variation",
keywords = "fraud",
keywords = "collaborative research",
abstract = "In 1980–1981 the National Heart, Lung, and Blood Institute (NHLBI) sponsored a multicenter study of animal models for protection of ischemic myocardium (AMPIM). After data collection for the study was complete, problems were identified with data from one of the labs. This article deals with the issue of statistical evidence versus hard evidence as well as the process of crossing the boundary from suspicion to certainty regarding the nature and extent of the problem."
}

# misconduct
# ffp
# data fabrication
# ddfab
@Article{Akhtar-Danesh2003,
author="Akhtar-Danesh, Noori
and Dehghan-Kooshkghazi, Mahshid",
title="How does correlation structure differ between real and fabricated data-sets?",
journal="BMC Medical Research Methodology",
year="2003",
volume="3",
number="1",
pages="1--9",
abstract="Misconduct in medical research has been the subject of many papers in recent years. Among different types of misconduct, data fabrication might be considered as one of the most severe cases. There have been some arguments that correlation coefficients in fabricated data-sets are usually greater than that found in real data-sets. We aim to study the differences between real and fabricated data-sets in term of the association between two variables.",
issn="1471-2288",
doi="10.1186/1471-2288-3-18",
url="http://dx.doi.org/10.1186/1471-2288-3-18"
}

@MISC{Hartgerink2016-lg,
  title     = "{688,112 statistical results: Content mining psychology articles
               for statistical test results}",
  author    = "Hartgerink, C H J",
  publisher = "Data Archiving and Networked Services (DANS)",
  year      =  2016,
  url       = "http://dx.doi.org/10.17026/dans-2cm-v9j9",
  doi       = "10.17026/dans-2cm-v9j9"
}

@article{Goodman2008135,
title = "A Dirty Dozen: Twelve P-Value Misconceptions ",
journal = "Seminars in Hematology ",
volume = "45",
number = "3",
pages = "135 - 140",
year = "2008",
note = "Interpretation of Quantitative Research ",
issn = "0037-1963",
doi = "http://dx.doi.org/10.1053/j.seminhematol.2008.04.003",
url = "http://www.sciencedirect.com/science/article/pii/S0037196308000620",
author = "Steven Goodman",
abstract = "The P value is a measure of statistical evidence that appears in virtually all medical research papers. Its interpretation is made extraordinarily difficult because it is not part of any formal system of statistical inference. As a result, the P value's inferential meaning is widely and often wildly misconstrued, a fact that has been pointed out in innumerable papers and books appearing since at least the 1940s. This commentary reviews a dozen of these common misinterpretations and explains why each is wrong. It also reviews the possible consequences of these improper understandings or representations of its meaning. Finally, it contrasts the P value with its Bayesian counterpart, the Bayes' factor, which has virtually all of the desirable properties of an evidential measure that the P value lacks, most notably interpretability. The most serious consequence of this array of P-value misconceptions is the false belief that the probability of a conclusion being in error can be calculated from the data in a single experiment without reference to external evidence or the plausibility of the underlying mechanism. "
}

@article{Altman_1995,
	doi = {10.1136/bmj.311.7003.485},
	url = {http://dx.doi.org/10.1136/bmj.311.7003.485},
	year = 1995,
	month = {aug},
	publisher = {{BMJ}},
	volume = {311},
	number = {7003},
	pages = {485--485},
	author = {D. G Altman and J M. Bland},
	title = {Statistics notes: Absence of evidence is not evidence of absence},
	journal = {{BMJ}}
}

@article{doi:10.1148/radiology.143.1.7063747,
author = {J A Hanley and B J McNeil},
title = {The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
journal = {Radiology},
volume = {143},
number = {1},
pages = {29-36},
year = {1982},
doi = {10.1148/radiology.143.1.7063747},
    note ={PMID: 7063747},

URL = { 
        http://dx.doi.org/10.1148/radiology.143.1.7063747
    
},
eprint = { 
        http://dx.doi.org/10.1148/radiology.143.1.7063747
    
}
,
    abstract = { A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the "rating" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques. }
}

@article{Ebersole201668,
title = "Many Labs 3: Evaluating participant pool quality across the academic semester via replication ",
journal = "Journal of Experimental Social Psychology ",
volume = "67",
number = "",
pages = "68 - 82",
year = "2016",
note = "Special Issue: Confirmatory ",
issn = "0022-1031",
doi = "http://dx.doi.org/10.1016/j.jesp.2015.10.012",
url = "http://www.sciencedirect.com/science/article/pii/S0022103115300123",
author = "Charles R. Ebersole and Olivia E. Atherton and Aimee L. Belanger and Hayley M. Skulborstad and Jill M. Allen and Jonathan B. Banks and Erica Baranski and Michael J. Bernstein and Diane B.V. Bonfiglio and Leanne Boucher and Elizabeth R. Brown and Nancy I. Budiman and Athena H. Cairo and Colin A. Capaldi and Christopher R. Chartier and Joanne M. Chung and David C. Cicero and Jennifer A. Coleman and John G. Conway and William E. Davis and Thierry Devos and Melody M. Fletcher and Komi German and Jon E. Grahe and Anthony D. Hermann and Joshua A. Hicks and Nathan Honeycutt and Brandon Humphrey and Matthew Janus and David J. Johnson and Jennifer A. Joy-Gaba and Hannah Juzeler and Ashley Keres and Diana Kinney and Jacqeline Kirshenbaum and Richard A. Klein and Richard E. Lucas and Christopher J.N. Lustgraaf and Daniel Martin and Madhavi Menon and Mitchell Metzger and Jaclyn M. Moloney and Patrick J. Morse and Radmila Prislin and Timothy Razza and Daniel E. Re and Nicholas O. Rule and Donald F. Sacco and Kyle Sauerberger and Emily Shrider and Megan Shultz and Courtney Siemsen and Karin Sobocko and R. Weylin Sternglanz and Amy Summerville and Konstantin O. Tskhay and Zack van Allen and Leigh Ann Vaughn and Ryan J. Walker and Ashley Weinberg and John Paul Wilson and James H. Wirth and Jessica Wortman and Brian A. Nosek",
keywords = "Social psychology",
keywords = "Cognitive psychology",
keywords = "Replication",
keywords = "Participant pool",
keywords = "Individual differences",
keywords = "Sampling effects",
keywords = "Situational effects ",
abstract = "Abstract The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects. "
}

